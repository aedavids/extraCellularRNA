{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "960efdb6",
   "metadata": {},
   "source": [
    "# clean up /scratch/aedavids/GTExData/countsGroupedByGene column names\n",
    "\n",
    "We are having trouble getting spark to create the final count matrix and estimated scaling factors. As a work around we had spark save grouped counts. The columnn names are wrong. instead of  'sum(GTEX-1117F-0226-SM-5GZZ7)' we want  'GTEX-1117F-0226-SM-5GZZ7'\n",
    "\n",
    "The GTExTrainNumReadsMatrix.tsv was to big to be groupedByGene in one spark batch job. It was broken into smaller batches. We split based on number of columns. These part files will need to be sorted before they can be pasted back together\n",
    "\n",
    "```\n",
    "Aedavids@ucsc.edu\n",
    "1/24/22\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23baaf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b9d3a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPARK_HOME=\"../../sparkBin/spark-3.1.2-bin-hadoop3.2\"\n",
    "import findspark\n",
    "findspark.init( SPARK_HOME )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66e1e612",
   "metadata": {},
   "outputs": [],
   "source": [
    "from   pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f616586c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/private/home/aedavids/extraCellularRNA/sparkBin/spark-3.1.2-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/03/24 15:18:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"cleanup\") \\\n",
    "    .config(\"spark.driver.memory\", \"15g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db9a27a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixSumColNames( df ):\n",
    "    '''\n",
    "    fix column names. the should be sample names\n",
    "    # 'sum(ctrl_1)'' should be 'ctrl_1'\n",
    "    '''\n",
    "\n",
    "    # skip the the geneId column It is value like gene_2\n",
    "    oldNames = df.columns[1:]\n",
    "\n",
    "    sampleNames = ( ( c, c.replace( 'sum(', '' ).replace( ')', '' ) )  for c in oldNames )\n",
    "    # print(*sampleColNames)\n",
    "\n",
    "    retDF = df.select( col( 'geneId' ),\n",
    "                           *( ( col( c ).alias( a ) for c, a in sampleNames ) ) )\n",
    "\n",
    "\n",
    "    return( retDF )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2879b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixSumColNamesAndSort(df):\n",
    "    retDF = fixSumColNames( df ).orderBy('geneId')\n",
    "    return retDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e0fc59",
   "metadata": {},
   "source": [
    "# fix the Training batch files\n",
    "Use unix paste to combine the batches together. \n",
    "\n",
    "## first batch\n",
    "we need to keep the geneId column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c30196ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scratch-aedavids/GTExData/data/deseq/download/trainNumReadsMatrixBatch1/estimateScalingFactorsCLI.py/countsGroupedByGene/part-00000-395d0bc2-2daf-4c17-afc1-c46e5a50f51b-c000.csv',\n",
       " 'scratch-aedavids/GTExData/data/deseq/download/trainNumReadsMatrixBatch2/estimateScalingFactorsCLI.py/countsGroupedByGene/part-00000-ef350e82-35a2-436e-90b1-b444b91ee019-c000.csv',\n",
       " 'scratch-aedavids/GTExData/data/deseq/download/trainNumReadsMatrixBatch3/estimateScalingFactorsCLI.py/countsGroupedByGene/part-00000-8a7c823c-c70d-4080-ae85-62354f10ce9c-c000.csv',\n",
       " 'scratch-aedavids/GTExData/data/deseq/download/trainNumReadsMatrixBatch4/estimateScalingFactorsCLI.py/countsGroupedByGene/part-00000-d6da8081-5fd6-471a-88e3-79fcfb37fd29-c000.csv']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rootDir = \"scratch-aedavids/GTExData/data/deseq\"\n",
    "x = \"/download/trainNumReadsMatrixBatch*/estimateScalingFactorsCLI.py\"\n",
    "y = \"/countsGroupedByGene/part*.csv\"\n",
    "p = rootDir + x + y\n",
    "trainingPartFiles = !! ls $p | sort\n",
    "trainingPartFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04ff5336",
   "metadata": {},
   "outputs": [],
   "source": [
    "firstTrainingBatchPartfile = trainingPartFiles[0] # leave geneId column\n",
    "remainingTrainingBatchPartFiles = trainingPartFiles[1:] # remove geneId column after sort and before save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2970c051",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleanTrainBatch1DF numRows:74777 numCols:2650\n",
      "['geneId', 'GTEX-1117F-0226-SM-5GZZ7', 'GTEX-1117F-0526-SM-5EGHJ']\n",
      "['GTEX-14C5O-2526-SM-73KU2', 'GTEX-14C5O-2626-SM-5RQI5', 'GTEX-14C5O-2826-SM-5RQI6']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------------+------------------------+\n",
      "|    geneId|GTEX-1117F-0226-SM-5GZZ7|GTEX-1117F-0526-SM-5EGHJ|\n",
      "+----------+------------------------+------------------------+\n",
      "|      (A)n|                       9|                       1|\n",
      "|    (AAA)n|                       0|                       0|\n",
      "|(AAAAAAC)n|                       0|                       0|\n",
      "+----------+------------------------+------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/24 15:18:26 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "22/03/24 15:18:27 WARN DAGScheduler: Broadcasting large task binary with size 1061.4 KiB\n",
      "22/03/24 15:18:40 WARN DAGScheduler: Broadcasting large task binary with size 1072.8 KiB\n",
      "22/03/24 15:18:48 WARN DAGScheduler: Broadcasting large task binary with size 1245.3 KiB\n",
      "[Stage 9:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrote: scratch-aedavids/GTExData/data/deseq/GTExTrainGroupByGenesCountMatrix.1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def firstTrainingBatch():\n",
    "    trainingBatch1DF = spark.read.load( firstTrainingBatchPartfile, \n",
    "                                        format=\"csv\", \n",
    "                                        header=\"true\" ) \n",
    "\n",
    "    cleanTrainBatch1DF = fixSumColNamesAndSort(trainingBatch1DF)\n",
    "    print(\"cleanTrainBatch1DF numRows:{} numCols:{}\".format(cleanTrainBatch1DF.count(), \n",
    "                                                            len(cleanTrainBatch1DF.columns)))\n",
    "    print(cleanTrainBatch1DF.columns[0:3])\n",
    "    print(cleanTrainBatch1DF.columns[-3:])\n",
    "\n",
    "    len(cleanTrainBatch1DF.columns)\n",
    "\n",
    "    cleanTrainBatch1DF.select( cleanTrainBatch1DF.columns[0:3] ).show(3)\n",
    "\n",
    "    cleanTrainBatch1DF.select( cleanTrainBatch1DF.columns[0:3] ).tail(3)\n",
    "\n",
    "    batchStr = str(1)\n",
    "    outFile =  rootDir + \"/\" \"GTExTrainGroupByGenesCountMatrix.\" + batchStr + \".csv\"\n",
    "    cleanTrainBatch1DF.coalesce(1).write.csv( outFile, mode='overwrite', header=True)\n",
    "    print(\"wrote: {}\".format(outFile))\n",
    "    \n",
    "firstTrainingBatch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77b1ccb",
   "metadata": {},
   "source": [
    "## sort and clean up the remaining GTExTraining groupby partfiles\n",
    "remove the geneId column before saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58a59041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********* scratch-aedavids/GTExData/data/deseq/download/trainNumReadsMatrixBatch2/estimateScalingFactorsCLI.py/countsGroupedByGene/part-00000-ef350e82-35a2-436e-90b1-b444b91ee019-c000.csv\n",
      "saveDF numRows:74777 numCols:2651\n",
      "['GTEX-14C5O-2926-SM-5RQI1', 'GTEX-14C5O-3026-SM-5YYB2', 'GTEX-14C5O-3126-SM-664NK']\n",
      "['GTEX-1H3VY-2226-SM-9WYU7', 'GTEX-1H3VY-2326-SM-9WG81', 'GTEX-1H3VY-2426-SM-9WPPD']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/24 15:19:42 WARN DAGScheduler: Broadcasting large task binary with size 1061.6 KiB\n",
      "22/03/24 15:19:57 WARN DAGScheduler: Broadcasting large task binary with size 1073.0 KiB\n",
      "22/03/24 15:20:08 WARN DAGScheduler: Broadcasting large task binary with size 1259.1 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrote: scratch-aedavids/GTExData/data/deseq/GTExTrainGroupByGenesCountMatrix.2.csv\n",
      "\n",
      "********* scratch-aedavids/GTExData/data/deseq/download/trainNumReadsMatrixBatch3/estimateScalingFactorsCLI.py/countsGroupedByGene/part-00000-8a7c823c-c70d-4080-ae85-62354f10ce9c-c000.csv\n",
      "saveDF numRows:74777 numCols:2651\n",
      "['GTEX-1H3VY-2726-SM-A96TL', 'GTEX-1H4P4-0006-SM-AHZ24', 'GTEX-1H4P4-0011-R10b-SM-CE6SG']\n",
      "['GTEX-QESD-1626-SM-2S1RB', 'GTEX-QESD-1726-SM-2S1R7', 'GTEX-QESD-2026-SM-447BI']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/24 15:21:55 WARN DAGScheduler: Broadcasting large task binary with size 1058.4 KiB\n",
      "22/03/24 15:22:03 WARN DAGScheduler: Broadcasting large task binary with size 1069.8 KiB\n",
      "22/03/24 15:22:12 WARN DAGScheduler: Broadcasting large task binary with size 1255.9 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrote: scratch-aedavids/GTExData/data/deseq/GTExTrainGroupByGenesCountMatrix.3.csv\n",
      "\n",
      "********* scratch-aedavids/GTExData/data/deseq/download/trainNumReadsMatrixBatch4/estimateScalingFactorsCLI.py/countsGroupedByGene/part-00000-d6da8081-5fd6-471a-88e3-79fcfb37fd29-c000.csv\n",
      "saveDF numRows:74777 numCols:2460\n",
      "['GTEX-QLQ7-0005-SM-2S1QP', 'GTEX-QLQ7-0008-SM-447AW', 'GTEX-QLQ7-0526-SM-2I5G3']\n",
      "['GTEX-ZZPU-1126-SM-5N9CW', 'GTEX-ZZPU-1326-SM-5GZWS', 'GTEX-ZZPU-2726-SM-5NQ8O']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/24 15:24:12 WARN DAGScheduler: Broadcasting large task binary with size 1172.5 KiB\n",
      "[Stage 27:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrote: scratch-aedavids/GTExData/data/deseq/GTExTrainGroupByGenesCountMatrix.4.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def remainingTrainingBatch():\n",
    "    for i in range(len(remainingTrainingBatchPartFiles)):\n",
    "        batch = remainingTrainingBatchPartFiles[i]\n",
    "        print(\"\\n********* {}\".format(batch))\n",
    "        trainingBatchDF = spark.read.load( batch, \n",
    "                                        format=\"csv\", \n",
    "                                        header=\"true\" )    \n",
    "        cleanTrainBatchDF = fixSumColNamesAndSort(trainingBatchDF)\n",
    "        # remove the the geneId column. It is the first column\n",
    "        saveDF = cleanTrainBatchDF.select( cleanTrainBatchDF.columns[1:] )\n",
    "\n",
    "        print(\"saveDF numRows:{} numCols:{}\".format(saveDF.count(), \n",
    "                                                            len(saveDF.columns)))\n",
    "        print(saveDF.columns[0:3])\n",
    "        print(saveDF.columns[-3:])\n",
    "\n",
    "        batchStr = str(i + 2)\n",
    "        outFile =  rootDir + \"/\" \"GTExTrainGroupByGenesCountMatrix.\" + batchStr + \".csv\"\n",
    "        saveDF.coalesce(1).write.csv( outFile, mode='overwrite', header=True)\n",
    "        print(\"wrote: {}\".format(outFile))\n",
    "        \n",
    "remainingTrainingBatch()        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1eeab68",
   "metadata": {},
   "source": [
    "# Fix the validation and test group by count matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ab51f5b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scratch-aedavids/GTExData/data/deseq/download/validate/estimateScalingFactorsCLI.py/countsGroupedByGene/part-00000-78f365fd-60bf-4b2d-a663-0a9a36eb7bbb-c000.csv\n",
      "scratch-aedavids/GTExData/data/deseq/download/test/estimateScalingFactorsCLI.py/countsGroupedByGene/part-00000-ca9d3ff0-1746-4fec-bf7f-f0d29ccda5ba-c000.csv\n"
     ]
    }
   ],
   "source": [
    "validatePartFile = !! ls $rootDir/download/validate/estimateScalingFactorsCLI.py/countsGroupedByGene/part*.csv\n",
    "validatePartFile = validatePartFile[0]\n",
    "\n",
    "testPartFile = !! ls $rootDir/download/test/estimateScalingFactorsCLI.py/countsGroupedByGene/part*.csv\n",
    "testPartFile = testPartFile[0]\n",
    "\n",
    "print(validatePartFile)\n",
    "print(testPartFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "df6ff330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********* scratch-aedavids/GTExData/data/deseq/download/validate/estimateScalingFactorsCLI.py/countsGroupedByGene/part-00000-78f365fd-60bf-4b2d-a663-0a9a36eb7bbb-c000.csv\n",
      "saveDF numRows:74777 numCols:3472\n",
      "['geneId', 'GTEX-1117F-0426-SM-5EGHI', 'GTEX-1117F-0726-SM-5GIEN']\n",
      "['GTEX-1B932-0226-SM-7PBYH', 'GTEX-1B932-0326-SM-7IGMV', 'GTEX-1B932-0426-SM-73KX4']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/24 15:45:42 WARN DAGScheduler: Broadcasting large task binary with size 1385.8 KiB\n",
      "22/03/24 15:46:11 WARN DAGScheduler: Broadcasting large task binary with size 1397.3 KiB\n",
      "22/03/24 15:46:25 WARN DAGScheduler: Broadcasting large task binary with size 1573.0 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrote: scratch-aedavids/GTExData/data/deseq/GTExValidateGroupByGenesCountMatrix.csv\n",
      "\n",
      "********* scratch-aedavids/GTExData/data/deseq/download/test/estimateScalingFactorsCLI.py/countsGroupedByGene/part-00000-ca9d3ff0-1746-4fec-bf7f-f0d29ccda5ba-c000.csv\n",
      "saveDF numRows:74777 numCols:3472\n",
      "['geneId', 'GTEX-1117F-0226-SM-5GZZ7', 'GTEX-1117F-0526-SM-5EGHJ']\n",
      "['GTEX-1B932-0726-SM-731EY', 'GTEX-1B932-0826-SM-73KXG', 'GTEX-1B932-0926-SM-73KUP']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/24 15:47:28 WARN DAGScheduler: Broadcasting large task binary with size 1385.9 KiB\n",
      "22/03/24 15:47:39 WARN DAGScheduler: Broadcasting large task binary with size 1397.3 KiB\n",
      "22/03/24 15:47:52 WARN DAGScheduler: Broadcasting large task binary with size 1573.0 KiB\n",
      "[Stage 39:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrote: scratch-aedavids/GTExData/data/deseq/GTExTestGroupByGenesCountMatrix.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def fixBatch( partFile, outFileName):\n",
    "    print(\"\\n********* {}\".format(partFile))\n",
    "    trainingBatchDF = spark.read.load( partFile, \n",
    "                                    format=\"csv\", \n",
    "                                    header=\"true\" )    \n",
    "    \n",
    "    saveDF = fixSumColNamesAndSort(trainingBatchDF)\n",
    "    print(\"saveDF numRows:{} numCols:{}\".format(saveDF.count(), \n",
    "                                                        len(saveDF.columns)))\n",
    "    print(saveDF.columns[0:3])\n",
    "    print(saveDF.columns[-3:])\n",
    "\n",
    "    outFile =  rootDir + \"/\" + outFileName + \".csv\"\n",
    "    saveDF.coalesce(1).write.csv( outFile, mode='overwrite', header=True)\n",
    "    print(\"wrote: {}\".format(outFile))\n",
    "    \n",
    "fixBatch( validatePartFile, \"GTExValidateGroupByGenesCountMatrix\" )\n",
    "fixBatch( testPartFile, \"GTExTestGroupByGenesCountMatrix\" )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
