{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "atomic-offering",
   "metadata": {},
   "source": [
    "# Create DESeq2 master data sets\n",
    "\n",
    "create a master count matrix and master colData data\n",
    "\n",
    "ref: [sql-programming-guide.htm](https://spark.apache.org/docs/latest/sql-programming-guide.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "contained-command",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPARK_HOME=\"../../sparkBin/spark-3.1.2-bin-hadoop3.2\"\n",
    "import findspark\n",
    "findspark.init( SPARK_HOME )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "muslim-basket",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pathlib as Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "alert-native",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"DESeqMasterETL\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# .config(\"spark.some.config.option\", \"some-value\") \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "beginning-longitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as sqlFunc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "revolutionary-render",
   "metadata": {},
   "source": [
    "## load our mock terra data model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "private-terrace",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleFile = \"testData/sample.tsv\"\n",
    "samplePDF = pd.read_csv(sampleFile, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "joined-cargo",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ctrl.1.quant.sf',\n",
       " 'ctrl.2.quant.sf',\n",
       " 'ctrl.3.quant.sf',\n",
       " 'kras.1.quant.sf',\n",
       " 'kras.2.quant.sf',\n",
       " 'kras.3.quant.sf']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fileLst = samplePDF.loc[:, 'quant.sf'].tolist()\n",
    "fileLst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "living-shield",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre allocate slots to store data frames in\n",
    "quantSDFs = [None] * len(fileLst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "documented-dispatch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testData/ctrl.1.quant.sf\n",
      "testData/ctrl.2.quant.sf\n",
      "testData/ctrl.3.quant.sf\n",
      "testData/kras.1.quant.sf\n",
      "testData/kras.2.quant.sf\n",
      "testData/kras.3.quant.sf\n"
     ]
    }
   ],
   "source": [
    "quantSchema = \"`Name` STRING, `Length` INT, `EffectiveLength` DOUBLE, `Tmp` DOUBLE, `NumReads` DOUBLE \"\n",
    "\n",
    "for i in range( len(quantSDFs) ):\n",
    "    quantFile = \"testData/{}\".format( fileLst[i] )\n",
    "    print(quantFile)\n",
    "    df = spark.read.load( quantFile, format=\"csv\", sep=\"\\t\", \n",
    "                             schema=quantSchema, header=\"true\")\n",
    "    quantSDFs[i] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ecological-society",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Length: integer (nullable = true)\n",
      " |-- EffectiveLength: double (nullable = true)\n",
      " |-- Tmp: double (nullable = true)\n",
      " |-- NumReads: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "quantSDFs[0].printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "significant-hearts",
   "metadata": {},
   "source": [
    "## explore quant.sf file format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "lucky-korea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+---------------+---------+--------+\n",
      "|                Name|Length|EffectiveLength|      Tmp|NumReads|\n",
      "+--------------------+------+---------------+---------+--------+\n",
      "|ENST00000456328.2...|  1657|         1530.0|      0.0|     0.0|\n",
      "|ENST00000450305.2...|   632|          505.0|      0.0|     0.0|\n",
      "|ENST00000488147.1...|  1351|         1224.0|      0.0|     0.0|\n",
      "|ENST00000619216.1...|    68|           15.0|      0.0|     0.0|\n",
      "|ENST00000473358.1...|   712|          585.0|      0.0|     0.0|\n",
      "|ENST00000469289.1...|   535|          408.0|      0.0|     0.0|\n",
      "|ENST00000607096.1...|   138|           55.0|      0.0|     0.0|\n",
      "|ENST00000417324.1...|  1187|         1060.0|      0.0|     0.0|\n",
      "|ENST00000461467.1...|   590|         885.84| 5.813169|    4.93|\n",
      "|ENST00000606857.1...|   840|          713.0|      0.0|     0.0|\n",
      "|ENST00000642116.1...|  1414|         1287.0|      0.0|     0.0|\n",
      "|ENST00000492842.2...|   939|          812.0|      0.0|     0.0|\n",
      "|ENST00000641515.2...|  2618|         2491.0|      0.0|     0.0|\n",
      "|ENST00000335137.4...|  1054|         348.69| 3.815162|   1.274|\n",
      "|ENST00000466430.5...|  2748|       3036.339|10.847308|  31.533|\n",
      "|ENST00000477740.5...|   491|          365.0|      0.0|     0.0|\n",
      "|ENST00000471248.1...|   629|        708.813|61.221885|  41.547|\n",
      "|ENST00000610542.1...|   723|          596.0|      0.0|     0.0|\n",
      "|ENST00000453576.2...|   336|          214.0|      0.0|     0.0|\n",
      "+--------------------+------+---------------+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "quantSDFs[0].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arranged-discrimination",
   "metadata": {},
   "source": [
    "# create master count spark data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "demanding-kingston",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ctrl_1', 'ctrl_2', 'ctrl_3', 'kras_1', 'kras_2', 'kras_3']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampleNamesLst = [None] * len(fileLst)\n",
    "for i in range( len(fileLst) ):\n",
    "    fileName = fileLst[i]\n",
    "    sampleNamesLst[i] = fileName.replace(\".quant.sf\", \"\") \n",
    "    sampleNamesLst[i] = sampleNamesLst[i].replace(\".\", \"_\")\n",
    "sampleNamesLst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "presidential-league",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize master counts spark data fram\n",
    "firstSampleName = sampleNamesLst[0]\n",
    "masterCountSDF = quantSDFs[0].select( [\"Name\", \"NumReads\"] ).withColumnRenamed( \"NumReads\", firstSampleName )\n",
    "\n",
    "# Register the DataFrame as a SQL temporary view\n",
    "masterCountSDF.createOrReplaceTempView(\"masterCount\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "political-azerbaijan",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range( 1, len(sampleNamesLst)):\n",
    "    sampleName = sampleNamesLst[i]\n",
    "    #print(sampleName)\n",
    "    \n",
    "    # select the key and counts from the sample. \n",
    "    sampleSDF = quantSDFs[i].select( [\"Name\", \"NumReads\", ] ).withColumnRenamed( \"NumReads\", sampleName )\n",
    "    sampleSDF.createOrReplaceTempView(\"sample\")\n",
    "    \n",
    "    sqlStmt = ' select mc.*, {}  \\n\\\n",
    "                      from \\n\\\n",
    "                          masterCount as mc, \\n\\\n",
    "                          sample  \\n\\\n",
    "                      where \\n\\\n",
    "                          mc.Name == sample.Name  \\n'.format(sampleName)\n",
    "    #print(sqlStmt)\n",
    "    \n",
    "    masterCountSDF = spark.sql( sqlStmt )\n",
    "    masterCountSDF.createOrReplaceTempView(\"masterCount\")\n",
    "    #masterCountSDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heated-physics",
   "metadata": {},
   "source": [
    "# display the query plan. \n",
    "Spark does a lot of optimization under the cover\n",
    "\n",
    "spark has several join strategies. I list 2 common ones bellow\n",
    "\n",
    "I let it pick what ever it thought was best. I plan to check out query plan on\n",
    "real data before causing it to execute\n",
    "\n",
    "## Broadcast Hash Join\n",
    "Also known as a map-side-only join, the broadcast hash join is employed when two\n",
    "data sets, one small (fitting in the driver’s and executor’s memory) and another large\n",
    "enough to ideally be spared from movement, need to be joined over certain conditions\n",
    "or columns.\n",
    "\n",
    "## Shuffle Sort Merge Join\n",
    "The sort-merge algorithm is an efficient way to merge two large data sets over a common\n",
    "key that is sortable, unique, and can be assigned to or stored in the same partition—\n",
    "that is, two data sets with a common hashable key that end up being on the\n",
    "same partition. From Spark’s perspective, this means that all rows within each data set\n",
    "with the same key are hashed on the same partition on the same executor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "progressive-sunglasses",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "* Project (33)\n",
      "+- * BroadcastHashJoin Inner BuildRight (32)\n",
      "   :- * Project (27)\n",
      "   :  +- * BroadcastHashJoin Inner BuildRight (26)\n",
      "   :     :- * Project (21)\n",
      "   :     :  +- * BroadcastHashJoin Inner BuildRight (20)\n",
      "   :     :     :- * Project (15)\n",
      "   :     :     :  +- * BroadcastHashJoin Inner BuildRight (14)\n",
      "   :     :     :     :- * Project (9)\n",
      "   :     :     :     :  +- * BroadcastHashJoin Inner BuildRight (8)\n",
      "   :     :     :     :     :- * Project (3)\n",
      "   :     :     :     :     :  +- * Filter (2)\n",
      "   :     :     :     :     :     +- Scan csv  (1)\n",
      "   :     :     :     :     +- BroadcastExchange (7)\n",
      "   :     :     :     :        +- * Project (6)\n",
      "   :     :     :     :           +- * Filter (5)\n",
      "   :     :     :     :              +- Scan csv  (4)\n",
      "   :     :     :     +- BroadcastExchange (13)\n",
      "   :     :     :        +- * Project (12)\n",
      "   :     :     :           +- * Filter (11)\n",
      "   :     :     :              +- Scan csv  (10)\n",
      "   :     :     +- BroadcastExchange (19)\n",
      "   :     :        +- * Project (18)\n",
      "   :     :           +- * Filter (17)\n",
      "   :     :              +- Scan csv  (16)\n",
      "   :     +- BroadcastExchange (25)\n",
      "   :        +- * Project (24)\n",
      "   :           +- * Filter (23)\n",
      "   :              +- Scan csv  (22)\n",
      "   +- BroadcastExchange (31)\n",
      "      +- * Project (30)\n",
      "         +- * Filter (29)\n",
      "            +- Scan csv  (28)\n",
      "\n",
      "\n",
      "(1) Scan csv \n",
      "Output [2]: [Name#0, NumReads#4]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [file:/private/home/aedavids/extraCellularRNA/juypterNotebooks/spark/testData/ctrl.1.quant.sf]\n",
      "PushedFilters: [IsNotNull(Name)]\n",
      "ReadSchema: struct<Name:string,NumReads:double>\n",
      "\n",
      "(2) Filter [codegen id : 6]\n",
      "Input [2]: [Name#0, NumReads#4]\n",
      "Condition : isnotnull(Name#0)\n",
      "\n",
      "(3) Project [codegen id : 6]\n",
      "Output [2]: [Name#0, NumReads#4 AS ctrl_1#88]\n",
      "Input [2]: [Name#0, NumReads#4]\n",
      "\n",
      "(4) Scan csv \n",
      "Output [2]: [Name#10, NumReads#14]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [file:/private/home/aedavids/extraCellularRNA/juypterNotebooks/spark/testData/ctrl.2.quant.sf]\n",
      "PushedFilters: [IsNotNull(Name)]\n",
      "ReadSchema: struct<Name:string,NumReads:double>\n",
      "\n",
      "(5) Filter [codegen id : 1]\n",
      "Input [2]: [Name#10, NumReads#14]\n",
      "Condition : isnotnull(Name#10)\n",
      "\n",
      "(6) Project [codegen id : 1]\n",
      "Output [2]: [Name#10, NumReads#14 AS ctrl_2#93]\n",
      "Input [2]: [Name#10, NumReads#14]\n",
      "\n",
      "(7) BroadcastExchange\n",
      "Input [2]: [Name#10, ctrl_2#93]\n",
      "Arguments: HashedRelationBroadcastMode(List(input[0, string, true]),false), [id=#158]\n",
      "\n",
      "(8) BroadcastHashJoin [codegen id : 6]\n",
      "Left keys [1]: [Name#0]\n",
      "Right keys [1]: [Name#10]\n",
      "Join condition: None\n",
      "\n",
      "(9) Project [codegen id : 6]\n",
      "Output [3]: [Name#0, ctrl_1#88, ctrl_2#93]\n",
      "Input [4]: [Name#0, ctrl_1#88, Name#10, ctrl_2#93]\n",
      "\n",
      "(10) Scan csv \n",
      "Output [2]: [Name#20, NumReads#24]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [file:/private/home/aedavids/extraCellularRNA/juypterNotebooks/spark/testData/ctrl.3.quant.sf]\n",
      "PushedFilters: [IsNotNull(Name)]\n",
      "ReadSchema: struct<Name:string,NumReads:double>\n",
      "\n",
      "(11) Filter [codegen id : 2]\n",
      "Input [2]: [Name#20, NumReads#24]\n",
      "Condition : isnotnull(Name#20)\n",
      "\n",
      "(12) Project [codegen id : 2]\n",
      "Output [2]: [Name#20, NumReads#24 AS ctrl_3#103]\n",
      "Input [2]: [Name#20, NumReads#24]\n",
      "\n",
      "(13) BroadcastExchange\n",
      "Input [2]: [Name#20, ctrl_3#103]\n",
      "Arguments: HashedRelationBroadcastMode(List(input[0, string, true]),false), [id=#166]\n",
      "\n",
      "(14) BroadcastHashJoin [codegen id : 6]\n",
      "Left keys [1]: [Name#0]\n",
      "Right keys [1]: [Name#20]\n",
      "Join condition: None\n",
      "\n",
      "(15) Project [codegen id : 6]\n",
      "Output [4]: [Name#0, ctrl_1#88, ctrl_2#93, ctrl_3#103]\n",
      "Input [5]: [Name#0, ctrl_1#88, ctrl_2#93, Name#20, ctrl_3#103]\n",
      "\n",
      "(16) Scan csv \n",
      "Output [2]: [Name#30, NumReads#34]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [file:/private/home/aedavids/extraCellularRNA/juypterNotebooks/spark/testData/kras.1.quant.sf]\n",
      "PushedFilters: [IsNotNull(Name)]\n",
      "ReadSchema: struct<Name:string,NumReads:double>\n",
      "\n",
      "(17) Filter [codegen id : 3]\n",
      "Input [2]: [Name#30, NumReads#34]\n",
      "Condition : isnotnull(Name#30)\n",
      "\n",
      "(18) Project [codegen id : 3]\n",
      "Output [2]: [Name#30, NumReads#34 AS kras_1#117]\n",
      "Input [2]: [Name#30, NumReads#34]\n",
      "\n",
      "(19) BroadcastExchange\n",
      "Input [2]: [Name#30, kras_1#117]\n",
      "Arguments: HashedRelationBroadcastMode(List(input[0, string, true]),false), [id=#174]\n",
      "\n",
      "(20) BroadcastHashJoin [codegen id : 6]\n",
      "Left keys [1]: [Name#0]\n",
      "Right keys [1]: [Name#30]\n",
      "Join condition: None\n",
      "\n",
      "(21) Project [codegen id : 6]\n",
      "Output [5]: [Name#0, ctrl_1#88, ctrl_2#93, ctrl_3#103, kras_1#117]\n",
      "Input [6]: [Name#0, ctrl_1#88, ctrl_2#93, ctrl_3#103, Name#30, kras_1#117]\n",
      "\n",
      "(22) Scan csv \n",
      "Output [2]: [Name#40, NumReads#44]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [file:/private/home/aedavids/extraCellularRNA/juypterNotebooks/spark/testData/kras.2.quant.sf]\n",
      "PushedFilters: [IsNotNull(Name)]\n",
      "ReadSchema: struct<Name:string,NumReads:double>\n",
      "\n",
      "(23) Filter [codegen id : 4]\n",
      "Input [2]: [Name#40, NumReads#44]\n",
      "Condition : isnotnull(Name#40)\n",
      "\n",
      "(24) Project [codegen id : 4]\n",
      "Output [2]: [Name#40, NumReads#44 AS kras_2#133]\n",
      "Input [2]: [Name#40, NumReads#44]\n",
      "\n",
      "(25) BroadcastExchange\n",
      "Input [2]: [Name#40, kras_2#133]\n",
      "Arguments: HashedRelationBroadcastMode(List(input[0, string, true]),false), [id=#182]\n",
      "\n",
      "(26) BroadcastHashJoin [codegen id : 6]\n",
      "Left keys [1]: [Name#0]\n",
      "Right keys [1]: [Name#40]\n",
      "Join condition: None\n",
      "\n",
      "(27) Project [codegen id : 6]\n",
      "Output [6]: [Name#0, ctrl_1#88, ctrl_2#93, ctrl_3#103, kras_1#117, kras_2#133]\n",
      "Input [7]: [Name#0, ctrl_1#88, ctrl_2#93, ctrl_3#103, kras_1#117, Name#40, kras_2#133]\n",
      "\n",
      "(28) Scan csv \n",
      "Output [2]: [Name#50, NumReads#54]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [file:/private/home/aedavids/extraCellularRNA/juypterNotebooks/spark/testData/kras.3.quant.sf]\n",
      "PushedFilters: [IsNotNull(Name)]\n",
      "ReadSchema: struct<Name:string,NumReads:double>\n",
      "\n",
      "(29) Filter [codegen id : 5]\n",
      "Input [2]: [Name#50, NumReads#54]\n",
      "Condition : isnotnull(Name#50)\n",
      "\n",
      "(30) Project [codegen id : 5]\n",
      "Output [2]: [Name#50, NumReads#54 AS kras_3#151]\n",
      "Input [2]: [Name#50, NumReads#54]\n",
      "\n",
      "(31) BroadcastExchange\n",
      "Input [2]: [Name#50, kras_3#151]\n",
      "Arguments: HashedRelationBroadcastMode(List(input[0, string, true]),false), [id=#190]\n",
      "\n",
      "(32) BroadcastHashJoin [codegen id : 6]\n",
      "Left keys [1]: [Name#0]\n",
      "Right keys [1]: [Name#50]\n",
      "Join condition: None\n",
      "\n",
      "(33) Project [codegen id : 6]\n",
      "Output [7]: [Name#0, ctrl_1#88, ctrl_2#93, ctrl_3#103, kras_1#117, kras_2#133, kras_3#151]\n",
      "Input [8]: [Name#0, ctrl_1#88, ctrl_2#93, ctrl_3#103, kras_1#117, kras_2#133, Name#50, kras_3#151]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "masterCountSDF.explain( mode='formatted' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "familiar-contemporary",
   "metadata": {},
   "source": [
    "# invoke an action\n",
    "This will cause the optimized query plan to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "environmental-price",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+------+------+------+------+------+\n",
      "|                Name|ctrl_1|ctrl_2|ctrl_3|kras_1|kras_2|kras_3|\n",
      "+--------------------+------+------+------+------+------+------+\n",
      "|ENST00000456328.2...|   0.0| 2.006| 1.998|   0.0|   4.0|   0.0|\n",
      "|ENST00000450305.2...|   0.0|   0.0|   0.0|   0.0|   0.0|   0.0|\n",
      "|ENST00000488147.1...|   0.0|   0.0|   0.0|   0.0|   0.0|   0.0|\n",
      "|ENST00000619216.1...|   0.0|   0.0|   0.0|   0.0|   0.0|   0.0|\n",
      "|ENST00000473358.1...|   0.0|   0.0|   0.0|   0.0|   0.0|   0.0|\n",
      "|ENST00000469289.1...|   0.0|   0.0|   0.0|   0.0|   0.0|   0.0|\n",
      "|ENST00000607096.1...|   0.0|   0.0|   0.0|   0.0|   0.0|   0.0|\n",
      "|ENST00000417324.1...|   0.0|   0.0|   0.0|   0.0|   0.0|   0.0|\n",
      "|ENST00000461467.1...|  4.93|   0.0|   0.0|   0.0|   0.0|14.346|\n",
      "|ENST00000606857.1...|   0.0|   0.0|   0.0|   0.0|   0.0|   0.0|\n",
      "|ENST00000642116.1...|   0.0|   0.0|   0.0|   0.0|   0.0|   0.0|\n",
      "|ENST00000492842.2...|   0.0|   0.0|   0.0|   0.0|   0.0|   0.0|\n",
      "|ENST00000641515.2...|   0.0|   0.0|   0.0|   0.0|   0.0|   0.0|\n",
      "|ENST00000335137.4...| 1.274|   0.0|   0.0|   2.0|   0.0|   1.0|\n",
      "|ENST00000466430.5...|31.533| 8.964|13.366|   0.0|  44.2|51.384|\n",
      "|ENST00000477740.5...|   0.0|   0.0|   0.0|   0.0|   0.0|   0.0|\n",
      "|ENST00000471248.1...|41.547| 2.536|21.483|16.078|90.687|88.072|\n",
      "|ENST00000610542.1...|   0.0|   0.0|   0.0|   0.0|   0.0|   0.0|\n",
      "|ENST00000453576.2...|   0.0|   0.0|   0.0|   1.0|   0.0|   0.0|\n",
      "+--------------------+------+------+------+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "masterCountSDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southern-decision",
   "metadata": {},
   "source": [
    "# Save master count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "loose-hobby",
   "metadata": {},
   "outputs": [],
   "source": [
    "rootDir = Path.Path('./output')\n",
    "rootDir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "automated-navigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "masterOutFile = rootDir.joinpath( \"masterCount.tsv\" )\n",
    "# write to a single file\n",
    "masterCountSDF.write.csv( masterOutFile.as_posix(), mode='overwrite', sep='\\t', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "offensive-input",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+------+------+------+------+------+\n",
      "|                Name|ctrl_1|ctrl_2|ctrl_3|kras_1|kras_2|kras_3|\n",
      "+--------------------+------+------+------+------+------+------+\n",
      "|ENST00000456328.2...|   0.0| 2.006| 1.998|   0.0|   4.0|   0.0|\n",
      "|ENST00000450305.2...|   0.0|   0.0|   0.0|   0.0|   0.0|   0.0|\n",
      "|ENST00000488147.1...|   0.0|   0.0|   0.0|   0.0|   0.0|   0.0|\n",
      "|ENST00000619216.1...|   0.0|   0.0|   0.0|   0.0|   0.0|   0.0|\n",
      "|ENST00000473358.1...|   0.0|   0.0|   0.0|   0.0|   0.0|   0.0|\n",
      "|ENST00000469289.1...|   0.0|   0.0|   0.0|   0.0|   0.0|   0.0|\n",
      "|ENST00000607096.1...|   0.0|   0.0|   0.0|   0.0|   0.0|   0.0|\n",
      "|ENST00000417324.1...|   0.0|   0.0|   0.0|   0.0|   0.0|   0.0|\n",
      "|ENST00000461467.1...|  4.93|   0.0|   0.0|   0.0|   0.0|14.346|\n",
      "|ENST00000606857.1...|   0.0|   0.0|   0.0|   0.0|   0.0|   0.0|\n",
      "|ENST00000642116.1...|   0.0|   0.0|   0.0|   0.0|   0.0|   0.0|\n",
      "|ENST00000492842.2...|   0.0|   0.0|   0.0|   0.0|   0.0|   0.0|\n",
      "|ENST00000641515.2...|   0.0|   0.0|   0.0|   0.0|   0.0|   0.0|\n",
      "|ENST00000335137.4...| 1.274|   0.0|   0.0|   2.0|   0.0|   1.0|\n",
      "|ENST00000466430.5...|31.533| 8.964|13.366|   0.0|  44.2|51.384|\n",
      "|ENST00000477740.5...|   0.0|   0.0|   0.0|   0.0|   0.0|   0.0|\n",
      "|ENST00000471248.1...|41.547| 2.536|21.483|16.078|90.687|88.072|\n",
      "|ENST00000610542.1...|   0.0|   0.0|   0.0|   0.0|   0.0|   0.0|\n",
      "|ENST00000453576.2...|   0.0|   0.0|   0.0|   1.0|   0.0|   0.0|\n",
      "+--------------------+------+------+------+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# write to 3 files\n",
    "masterPartsOutFile = rootDir.joinpath( \"masterCountParts.tsv\" )\n",
    "\n",
    "numFiles = 3\n",
    "masterCountSDF.repartition(numFiles) \\\n",
    "    .write \\\n",
    "    .csv( masterPartsOutFile.as_posix(), mode='overwrite', sep='\\t', header=True)\n",
    "\n",
    "masterCountSDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "established-eleven",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masterCountSDF.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "restricted-overall",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
