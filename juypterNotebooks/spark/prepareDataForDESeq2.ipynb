{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "atomic-offering",
   "metadata": {},
   "source": [
    "# Prepare Data for DESeq2\n",
    "aedavids@ucsc.edu 10/24/21\n",
    "\n",
    "create a master count matrix and estimated scaling factors, we can use to test /private/home/aedavids/extraCellularRNA/terra/deseq/R/ DESeq scripts\n",
    "\n",
    "The reason we use spark for these tasks is that it required us to read the entire data set into memory. Something that would not be possible using a monolithic tool like DESeq2.\n",
    "\n",
    "ref: [sql-programming-guide.htm](https://spark.apache.org/docs/latest/sql-programming-guide.html)\n",
    "\n",
    "## <span style=\"color:red\">TODO make a production vesion so we can get better data and see if parts works or not\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "contained-command",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPARK_HOME=\"../../sparkBin/spark-3.1.2-bin-hadoop3.2\"\n",
    "import findspark\n",
    "findspark.init( SPARK_HOME )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "muslim-basket",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pathlib as Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "demanding-environment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/private/home/aedavids/extraCellularRNA/juypterNotebooks/spark\r\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "\n",
    "dataRoot = Path.Path(\"testData/sparkDESeqTest2\")\n",
    "rootOutDir = dataRoot.joinpath('output')\n",
    "rootOutDir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "alert-native",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"DESeqMasterETL\") \\\n",
    "    .config(\"spark.driver.memory\", \"15g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# .config(\"spark.some.config.option\", \"some-value\") \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "beginning-longitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as sqlFunc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annoying-store",
   "metadata": {},
   "source": [
    "# Step 1) Load Salmon Quant files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "asian-advertiser",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadSalmonQuantFiles( fileList, debug=False ):\n",
    "    \n",
    "    # pre allocate slots to store data frames in\n",
    "    quantSDFs = [None] * len(fileList)\n",
    "    \n",
    "    quantSchema = \"`Name` STRING, `Length` INT, `EffectiveLength` DOUBLE, `TPM` DOUBLE, `NumReads` DOUBLE \"\n",
    "\n",
    "    for i in range( len(quantSDFs) ):\n",
    "        quantFile =  fileList[i] \n",
    "        \n",
    "        if debug:\n",
    "            print(quantFile)\n",
    "            \n",
    "        df = spark.read.load( quantFile.as_posix(), format=\"csv\", sep=\"\\t\", \n",
    "                                 schema=quantSchema, header=\"true\")\n",
    "        quantSDFs[i] = df    \n",
    "        \n",
    "    return quantSDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "chubby-therapy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testData/sparkDESeqTest2/ctrl_1.quant.sf\n",
      "testData/sparkDESeqTest2/ctrl_2.quant.sf\n",
      "testData/sparkDESeqTest2/ctrl_3.quant.sf\n",
      "testData/sparkDESeqTest2/kras_1.quant.sf\n",
      "testData/sparkDESeqTest2/kras_2.quant.sf\n",
      "testData/sparkDESeqTest2/kras_3.quant.sf\n"
     ]
    }
   ],
   "source": [
    "# order is important, must match the colData row name\n",
    "sampleNames = [ \"ctrl_1\", \"ctrl_2\", \"ctrl_3\", \"kras_1\", \"kras_2\", \"kras_3\" ]\n",
    "# fileList = [ dataRoot + \"/\" + name + \".quant.sf\" for name in sampleNames ]\n",
    "fileList = [ dataRoot.joinpath(name + \".quant.sf\") for name in sampleNames ]\n",
    "\n",
    "quantSparkDFList = loadSalmonQuantFiles( fileList, debug=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "requested-berry",
   "metadata": {},
   "source": [
    "# Step 2) combine the quant files into a single count data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "quick-factory",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|   Name|ctrl_1|\n",
      "+-------+------+\n",
      "| txId_1|   0.0|\n",
      "| txId_2|  11.0|\n",
      "| txId_3|  12.0|\n",
      "| txId_4|  13.0|\n",
      "| txId_5|  14.0|\n",
      "| txId_6|  15.0|\n",
      "| txId_7|  16.0|\n",
      "| txId_8|  17.0|\n",
      "| txId_9|  18.0|\n",
      "|txId_10|  19.0|\n",
      "+-------+------+\n",
      "\n",
      "type(s1Col) = <class 'pyspark.sql.column.Column'>\n",
      "type(s1BCol) = <class 'pyspark.sql.column.Column'>\n"
     ]
    }
   ],
   "source": [
    "#https://stackoverflow.com/questions/55105363/pyspark-dataframe-column-reference-df-col-vs-dfcol-vs-f-colcol\n",
    "df1 = quantSparkDFList[0].select( [\"Name\", \"NumReads\"] ).withColumnRenamed( \"NumReads\", sampleNames[0] )\n",
    "df1.show()\n",
    "\n",
    "# this will not work with col names that contain space\n",
    "s1Col = df1.ctrl_1\n",
    "print(\"type(s1Col) = {}\".format(type(s1Col)) )\n",
    "\n",
    "s1BCol = df1[\"ctrl_1\"]\n",
    "print(\"type(s1BCol) = {}\".format(type(s1BCol)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "molecular-marker",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df1\n",
      "DataFrame[Name: string, ctrl_1: double]\n",
      "+-------+------+\n",
      "|   Name|ctrl_1|\n",
      "+-------+------+\n",
      "| txId_1|   0.0|\n",
      "| txId_2|  11.0|\n",
      "| txId_3|  12.0|\n",
      "| txId_4|  13.0|\n",
      "| txId_5|  14.0|\n",
      "| txId_6|  15.0|\n",
      "| txId_7|  16.0|\n",
      "| txId_8|  17.0|\n",
      "| txId_9|  18.0|\n",
      "|txId_10|  19.0|\n",
      "+-------+------+\n",
      "\n",
      "allDF\n",
      "DataFrame[Name: string, ctrl_1: double, x: string]\n",
      "+-------+------+---+\n",
      "|   Name|ctrl_1|  x|\n",
      "+-------+------+---+\n",
      "| txId_1|   0.0|abc|\n",
      "| txId_2|  11.0|abc|\n",
      "| txId_3|  12.0|abc|\n",
      "| txId_4|  13.0|abc|\n",
      "| txId_5|  14.0|abc|\n",
      "| txId_6|  15.0|abc|\n",
      "| txId_7|  16.0|abc|\n",
      "| txId_8|  17.0|abc|\n",
      "| txId_9|  18.0|abc|\n",
      "|txId_10|  19.0|abc|\n",
      "+-------+------+---+\n",
      "\n",
      "type(l2Col) = <class 'pyspark.sql.column.Column'>\n",
      "allDF2\n",
      "DataFrame[Name: string, ctrl_1: double, x: string, y: string]\n",
      "+-------+------+---+\n",
      "|   Name|ctrl_1|  y|\n",
      "+-------+------+---+\n",
      "| txId_1|   0.0|mno|\n",
      "| txId_2|  11.0|mno|\n",
      "| txId_3|  12.0|mno|\n",
      "| txId_4|  13.0|mno|\n",
      "| txId_5|  14.0|mno|\n",
      "| txId_6|  15.0|mno|\n",
      "| txId_7|  16.0|mno|\n",
      "| txId_8|  17.0|mno|\n",
      "| txId_9|  18.0|mno|\n",
      "|txId_10|  19.0|mno|\n",
      "+-------+------+---+\n",
      "\n",
      "allDF3\n",
      "DataFrame[Name: string, ctrl_1: double, x: string, y: string]\n",
      "+-------+------+---+---+\n",
      "|   Name|ctrl_1|  x|  y|\n",
      "+-------+------+---+---+\n",
      "| txId_1|   0.0|abc|mn0|\n",
      "| txId_2|  11.0|abc|mn0|\n",
      "| txId_3|  12.0|abc|mn0|\n",
      "| txId_4|  13.0|abc|mn0|\n",
      "| txId_5|  14.0|abc|mn0|\n",
      "| txId_6|  15.0|abc|mn0|\n",
      "| txId_7|  16.0|abc|mn0|\n",
      "| txId_8|  17.0|abc|mn0|\n",
      "| txId_9|  18.0|abc|mn0|\n",
      "|txId_10|  19.0|abc|mn0|\n",
      "+-------+------+---+---+\n",
      "\n",
      "\n",
      "df2\n",
      "DataFrame[Name: string, Length: int, EffectiveLength: double, TPM: double, NumReads: double]\n",
      "+-------+------+---------------+----+--------+\n",
      "|   Name|Length|EffectiveLength| TPM|NumReads|\n",
      "+-------+------+---------------+----+--------+\n",
      "| txId_1|  1500|         1234.5|12.1|     0.1|\n",
      "| txId_2|  1510|         1244.5|13.1|    11.1|\n",
      "| txId_3|  1520|         1254.5|14.1|    12.1|\n",
      "| txId_4|  1530|         1264.5|15.1|    13.1|\n",
      "| txId_5|  1540|         1274.5|16.1|    14.1|\n",
      "| txId_6|  1550|         1284.5|17.1|    15.1|\n",
      "| txId_7|  1560|         1294.5|18.1|    16.1|\n",
      "| txId_8|  1570|         1304.5|19.1|    17.1|\n",
      "| txId_9|  1580|         1314.5|20.1|    18.1|\n",
      "|txId_10|  1590|         1324.5|21.1|    19.1|\n",
      "+-------+------+---------------+----+--------+\n",
      "\n",
      "dfxxx\n",
      "DataFrame[Name: string, Length: int, EffectiveLength: double, TPM: double, ctrl_2: double]\n",
      "+-------+------+---------------+----+------+\n",
      "|   Name|Length|EffectiveLength| TPM|ctrl_2|\n",
      "+-------+------+---------------+----+------+\n",
      "| txId_1|  1500|         1234.5|12.1|   0.1|\n",
      "| txId_2|  1510|         1244.5|13.1|  11.1|\n",
      "| txId_3|  1520|         1254.5|14.1|  12.1|\n",
      "| txId_4|  1530|         1264.5|15.1|  13.1|\n",
      "| txId_5|  1540|         1274.5|16.1|  14.1|\n",
      "| txId_6|  1550|         1284.5|17.1|  15.1|\n",
      "| txId_7|  1560|         1294.5|18.1|  16.1|\n",
      "| txId_8|  1570|         1304.5|19.1|  17.1|\n",
      "| txId_9|  1580|         1314.5|20.1|  18.1|\n",
      "|txId_10|  1590|         1324.5|21.1|  19.1|\n",
      "+-------+------+---------------+----+------+\n",
      "\n",
      "type(s2Col) = <class 'pyspark.sql.column.Column'>\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "Resolved attribute(s) ctrl_2#3205 missing from Name#0,ctrl_1#3102 in operator !Project [Name#0, ctrl_1#3102, ctrl_2#3205].;\n!Project [Name#0, ctrl_1#3102, ctrl_2#3205]\n+- Project [Name#0, NumReads#4 AS ctrl_1#3102]\n   +- Project [Name#0, NumReads#4]\n      +- Relation[Name#0,Length#1,EffectiveLength#2,TPM#3,NumReads#4] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-374b15836356>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"type(s2Col) = {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms2Col\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0mallDF4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"*\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms2Col\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;31m#allDF4 = df1.select( [\"*\"] + [s2Col] )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;31m#allDF4 = df1.select( [\"*\", df2[\"NumReads\"] ] )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/extraCellularRNA/sparkBin/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   1667\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Alice'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Bob'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1668\u001b[0m         \"\"\"\n\u001b[0;32m-> 1669\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1670\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1671\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m../../sparkBin/spark-3.1.2-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/extraCellularRNA/sparkBin/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Resolved attribute(s) ctrl_2#3205 missing from Name#0,ctrl_1#3102 in operator !Project [Name#0, ctrl_1#3102, ctrl_2#3205].;\n!Project [Name#0, ctrl_1#3102, ctrl_2#3205]\n+- Project [Name#0, NumReads#4 AS ctrl_1#3102]\n   +- Project [Name#0, NumReads#4]\n      +- Relation[Name#0,Length#1,EffectiveLength#2,TPM#3,NumReads#4] csv\n"
     ]
    }
   ],
   "source": [
    "# https://mungingdata.com/pyspark/select-add-columns-withcolumn/\n",
    "df1 = quantSparkDFList[0].select( [\"Name\", \"NumReads\"] ).withColumnRenamed( \"NumReads\", sampleNames[0] )\n",
    "print(\"df1\")\n",
    "print(df1)\n",
    "df1.show()\n",
    "\n",
    "# use select to append a literal\n",
    "import pyspark.sql.functions as pyf\n",
    "allDF = df1.select( [\"*\", pyf.lit(\"abc\").alias(\"x\")] )\n",
    "print(\"allDF\")\n",
    "print(allDF)\n",
    "allDF.show()\n",
    "\n",
    "# make sure there is nothing strange about env col is created in\n",
    "l2Col = pyf.lit(\"mno\").alias(\"y\")\n",
    "print(\"type(l2Col) = {}\".format(type(l2Col)) )\n",
    "allDF2 = df1.select( [\"*\", l2Col] )\n",
    "print(\"allDF2\")\n",
    "print(allDF3)\n",
    "allDF2.show()\n",
    "\n",
    "\n",
    "# use select to append multiple literals\n",
    "allDF3 = df1.select( [\"*\", pyf.lit(\"abc\").alias(\"x\"), pyf.lit(\"mn0\").alias(\"y\")] )\n",
    "print(\"allDF3\")\n",
    "print(allDF3)\n",
    "allDF3.show()\n",
    "\n",
    "# append a col from another data frame\n",
    "print(\"\")\n",
    "df2 = quantSparkDFList[1]\n",
    "print(\"df2\")\n",
    "print(df2)\n",
    "df2.show()\n",
    "# s2Col = df2[\"NumReads\"] \n",
    "#s2Col = df2[\"NumReads\"].alias( 'ctrl_2' )\n",
    "dfxxx = df2.withColumnRenamed( \"NumReads\", \"ctrl_2\" )\n",
    "print(\"dfxxx\")\n",
    "print(dfxxx)\n",
    "dfxxx.show()\n",
    "# s2Col = dfxxx[\"ctrl_2\"]\n",
    "s2Col = dfxxx.ctrl_2\n",
    "print(\"type(s2Col) = {}\".format(type(s2Col)) )\n",
    "\n",
    "allDF4 = df1.select( [\"*\", s2Col] )\n",
    "#allDF4 = df1.select( [\"*\"] + [s2Col] )\n",
    "#allDF4 = df1.select( [\"*\", df2[\"NumReads\"] ] )\n",
    "# print(\"allDF4\")\n",
    "# allDF4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arranged-scholar",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createCountDF_AEDWIP( quantSparkDFList, sampleNamesList, debug=False ):\n",
    "    # initialize master counts spark data frame\n",
    "    firstSampleName = sampleNamesList[0]\n",
    "    rawCountsSDF = quantSparkDFList[0].select( [\"Name\", \"NumReads\"] )\\\n",
    "                            .withColumnRenamed( \"NumReads\", firstSampleName )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handmade-occasion",
   "metadata": {},
   "source": [
    "# expected out\n",
    "output from createCountDF_JOIN()\n",
    "```\n",
    "ctrl_1\n",
    "ctrl_2\n",
    "ctrl_3\n",
    "kras_1\n",
    "kras_2\n",
    "kras_3\n",
    "+-------+------+------+------+------+------+------+\n",
    "|Name   |ctrl_1|ctrl_2|ctrl_3|kras_1|kras_2|kras_3|\n",
    "+-------+------+------+------+------+------+------+\n",
    "|txId_1 |0.0   |0.1   |0.2   |0.0   |0.1   |0.2   |\n",
    "|txId_2 |11.0  |11.1  |11.2  |110.0 |110.1 |110.2 |\n",
    "|txId_3 |12.0  |12.1  |0.0   |120.0 |120.1 |120.2 |\n",
    "|txId_4 |13.0  |13.1  |13.2  |130.0 |130.1 |130.2 |\n",
    "|txId_5 |14.0  |14.1  |14.2  |140.0 |140.1 |140.2 |\n",
    "|txId_6 |15.0  |15.1  |15.2  |150.0 |150.1 |150.2 |\n",
    "|txId_7 |16.0  |16.1  |16.2  |160.0 |160.1 |160.2 |\n",
    "|txId_8 |17.0  |17.1  |17.2  |170.0 |170.1 |170.2 |\n",
    "|txId_9 |18.0  |18.1  |18.2  |180.0 |180.1 |180.2 |\n",
    "|txId_10|19.0  |19.1  |19.2  |190.0 |190.1 |190.2 |\n",
    "+-------+------+------+------+------+------+------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finnish-queen",
   "metadata": {},
   "outputs": [],
   "source": [
    "aedwp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adjacent-token",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createCountDF_JOIN( quantSparkDFList, sampleNamesList, debug=False ):\n",
    "    # initialize master counts spark data frame\n",
    "    firstSampleName = sampleNamesList[0]\n",
    "    rawCountsSDF = quantSparkDFList[0].select( [\"Name\", \"NumReads\"] )\\\n",
    "                            .withColumnRenamed( \"NumReads\", firstSampleName )\n",
    "\n",
    "    # Register the DataFrame as a SQL temporary view\n",
    "    rawCountsSDF.createOrReplaceTempView(\"rawCounts\")\n",
    "    \n",
    "    if debug:\n",
    "        print(firstSampleName)\n",
    "\n",
    "    for i in range( 1, len(sampleNamesList)):\n",
    "        sampleName = sampleNamesList[i]\n",
    "        \n",
    "        if debug:\n",
    "            print(sampleName)\n",
    "\n",
    "        # select the key and counts from the sample. \n",
    "        sampleSDF = quantSparkDFList[i].select( [\"Name\", \"NumReads\", ] )\\\n",
    "            .withColumnRenamed( \"NumReads\", sampleName )\n",
    "        \n",
    "        sampleSDF.createOrReplaceTempView(\"sample\")\n",
    "\n",
    "        sqlStmt = ' select rc.*, {}  \\n\\\n",
    "                          from \\n\\\n",
    "                              rawCounts as rc, \\n\\\n",
    "                              sample  \\n\\\n",
    "                          where \\n\\\n",
    "                              rc.Name == sample.Name  \\n'.format(sampleName)\n",
    "        #print(sqlStmt)\n",
    "\n",
    "        rawCountsSDF = spark.sql( sqlStmt )\n",
    "        rawCountsSDF.createOrReplaceTempView(\"rawCounts\")\n",
    "        #masterCountSDF.show()\n",
    "        \n",
    "    return rawCountsSDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "intimate-scheme",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ctrl_1\n",
      "ctrl_2\n",
      "ctrl_3\n",
      "kras_1\n",
      "kras_2\n",
      "kras_3\n",
      "+-------+------+------+------+------+------+------+\n",
      "|Name   |ctrl_1|ctrl_2|ctrl_3|kras_1|kras_2|kras_3|\n",
      "+-------+------+------+------+------+------+------+\n",
      "|txId_1 |0.0   |0.1   |0.2   |0.0   |0.1   |0.2   |\n",
      "|txId_2 |11.0  |11.1  |11.2  |110.0 |110.1 |110.2 |\n",
      "|txId_3 |12.0  |12.1  |0.0   |120.0 |120.1 |120.2 |\n",
      "|txId_4 |13.0  |13.1  |13.2  |130.0 |130.1 |130.2 |\n",
      "|txId_5 |14.0  |14.1  |14.2  |140.0 |140.1 |140.2 |\n",
      "|txId_6 |15.0  |15.1  |15.2  |150.0 |150.1 |150.2 |\n",
      "|txId_7 |16.0  |16.1  |16.2  |160.0 |160.1 |160.2 |\n",
      "|txId_8 |17.0  |17.1  |17.2  |170.0 |170.1 |170.2 |\n",
      "|txId_9 |18.0  |18.1  |18.2  |180.0 |180.1 |180.2 |\n",
      "|txId_10|19.0  |19.1  |19.2  |190.0 |190.1 |190.2 |\n",
      "+-------+------+------+------+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rawCountSparkDF = createCountDF( quantSparkDFList, sampleNames, debug=True )\n",
    "rawCountSparkDF.createOrReplaceTempView(\"rawCounts\")\n",
    "\n",
    "rawCountSparkDF.show( truncate=False )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italian-northwest",
   "metadata": {},
   "source": [
    "# Step 3, group by gene id and count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "twenty-monitor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def groupByGeneAndSum( txId2GeneIdFile, rawCountSDF, debug=False):\n",
    "    tx2geneSchema = \" `txId` STRING, `geneId` STRING \"\n",
    "    txt2geneSparkDF = spark.read.load( txId2GeneIdFile, format=\"csv\",  \n",
    "                                    schema=tx2geneSchema, header=\"false\")\n",
    "    if debug:\n",
    "        print(txt2geneSparkDF)\n",
    "        txt2geneSparkDF.show()\n",
    "    \n",
    "    txt2geneSparkDF.createOrReplaceTempView(\"txt2gene\")\n",
    "\n",
    "    sqlStmt = ' select geneId, rc.*  \\n\\\n",
    "                      from \\n\\\n",
    "                          rawCounts as rc, \\n\\\n",
    "                          txt2gene  \\n\\\n",
    "                      where \\n\\\n",
    "                          rc.Name == txt2gene.txId  \\n'\n",
    "\n",
    "    rawCountSDF = spark.sql( sqlStmt )\n",
    "    rawCountSDF.createOrReplaceTempView(\"rawCounts\")\n",
    "    \n",
    "    retSparkDF = rawCountSDF.groupBy(\"geneId\").sum()# do not sort it does not matter .sort(\"geneId\").show() \n",
    "    \n",
    "    return retSparkDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "pleasant-working",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[txId: string, geneId: string]\n",
      "+-------+------+\n",
      "|   txId|geneId|\n",
      "+-------+------+\n",
      "|   txId|geneId|\n",
      "| txId_1|gene_1|\n",
      "| txId_2|gene_2|\n",
      "| txId_3|gene_3|\n",
      "| txId_4|gene_4|\n",
      "| txId_5|gene_5|\n",
      "| txId_6|gene_6|\n",
      "| txId_7|gene_7|\n",
      "| txId_8|gene_8|\n",
      "| txId_9|gene_8|\n",
      "|txId_10|gene_8|\n",
      "+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csvFile = dataRoot.joinpath(\"txId2GeneId.csv\")\n",
    "countsSparkDF = groupByGeneAndSum(csvFile.as_posix(), rawCountSparkDF, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "diagnostic-poverty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txId_8, txId_9, txId_10 map to gene_8\n",
      "+------+-----------+------------------+------------------+-----------+-----------+-----------------+\n",
      "|geneId|sum(ctrl_1)|sum(ctrl_2)       |sum(ctrl_3)       |sum(kras_1)|sum(kras_2)|sum(kras_3)      |\n",
      "+------+-----------+------------------+------------------+-----------+-----------+-----------------+\n",
      "|gene_2|11.0       |11.1              |11.2              |110.0      |110.1      |110.2            |\n",
      "|gene_7|16.0       |16.1              |16.2              |160.0      |160.1      |160.2            |\n",
      "|gene_6|15.0       |15.1              |15.2              |150.0      |150.1      |150.2            |\n",
      "|gene_4|13.0       |13.1              |13.2              |130.0      |130.1      |130.2            |\n",
      "|gene_1|0.0        |0.1               |0.2               |0.0        |0.1        |0.2              |\n",
      "|gene_5|14.0       |14.1              |14.2              |140.0      |140.1      |140.2            |\n",
      "|gene_3|12.0       |12.1              |0.0               |120.0      |120.1      |120.2            |\n",
      "|gene_8|54.0       |54.300000000000004|54.599999999999994|540.0      |540.3      |540.5999999999999|\n",
      "+------+-----------+------------------+------------------+-----------+-----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"txId_8, txId_9, txId_10 map to gene_8\")\n",
    "countsSparkDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "colonial-template",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testCountsSparkDF( df ):\n",
    "    expectedRowStr = \"Row(geneId='gene_8', sum(ctrl_1)=54.0, sum(ctrl_2)=54.300000000000004, sum(ctrl_3)=54.599999999999994, sum(kras_1)=540.0, sum(kras_2)=540.3, sum(kras_3)=540.5999999999999)\"\n",
    "    retRow = df.filter( df.geneId == \"gene_8\").take(1)[0]\n",
    "    assert str(retRow) == expectedRowStr , \"group by gene count is wrong\"\n",
    "    \n",
    "testCountsSparkDF( countsSparkDF )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hungry-craps",
   "metadata": {},
   "source": [
    "# Step 4) convert counts to integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "geographic-hello",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+-----------+-----------+-----------+-----------+-----------+\n",
      "|geneId|sum(ctrl_1)|sum(ctrl_2)|sum(ctrl_3)|sum(kras_1)|sum(kras_2)|sum(kras_3)|\n",
      "+------+-----------+-----------+-----------+-----------+-----------+-----------+\n",
      "|gene_2|         11|         11|         11|        110|        110|        110|\n",
      "|gene_7|         16|         16|         16|        160|        160|        160|\n",
      "|gene_6|         15|         15|         15|        150|        150|        150|\n",
      "|gene_4|         13|         13|         13|        130|        130|        130|\n",
      "|gene_1|          0|          0|          0|          0|          0|          0|\n",
      "|gene_5|         14|         14|         14|        140|        140|        140|\n",
      "|gene_3|         12|         12|          0|        120|        120|        120|\n",
      "|gene_8|         54|         54|         55|        540|        540|        541|\n",
      "+------+-----------+-----------+-----------+-----------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, round\n",
    "# columns[1:], skip geneId col\n",
    "# https://mrpowers.medium.com/performing-operations-on-multiple-columns-in-a-pyspark-dataframe-36e97896c378\n",
    "# https://treyhunner.com/2018/10/asterisks-in-python-what-they-are-and-how-to-use-them/\n",
    "# countsSparkDF = countsSparkDF.select(*(col(c).cast(\"integer\").alias(c) for c in countsSparkDF.columns[1:]))\n",
    "\n",
    "# round before casting to int to match R implementation\n",
    "\n",
    "countsSparkDF = countsSparkDF.select( col('geneId') , \\\n",
    "        *(round(col(c)).alias(c) for c in countsSparkDF.columns[1:]) )\n",
    "\n",
    "countsSparkDF = countsSparkDF.select( col('geneId') , \\\n",
    "        *(col(c).cast(\"integer\").alias(c) for c in countsSparkDF.columns[1:]) )\n",
    "\n",
    "countsSparkDF.show()\n",
    "\n",
    "def testConvertToInt( df ):\n",
    "    expectedPDFDict = {'geneId': {0: 'gene_2', 1: 'gene_7', 2: 'gene_6', 3: 'gene_4', 4: 'gene_1', 5: 'gene_5', 6: 'gene_3', 7: 'gene_8'}, \n",
    "                       'sum(ctrl_1)': {0: 11, 1: 16, 2: 15, 3: 13, 4: 0, 5: 14, 6: 12, 7: 54}, \n",
    "                       'sum(ctrl_2)': {0: 11, 1: 16, 2: 15, 3: 13, 4: 0, 5: 14, 6: 12, 7: 54}, \n",
    "                       'sum(ctrl_3)': {0: 11, 1: 16, 2: 15, 3: 13, 4: 0, 5: 14, 6: 0, 7: 55}, \n",
    "                       'sum(kras_1)': {0: 110, 1: 160, 2: 150, 3: 130, 4: 0, 5: 140, 6: 120, 7: 540}, \n",
    "                       'sum(kras_2)': {0: 110, 1: 160, 2: 150, 3: 130, 4: 0, 5: 140, 6: 120, 7: 540}, \n",
    "                       'sum(kras_3)': {0: 110, 1: 160, 2: 150, 3: 130, 4: 0, 5: 140, 6: 120, 7: 541}}\n",
    "\n",
    "    expectedPDF = pd.DataFrame( expectedPDFDict )\n",
    "#     print(\"\\n expected\")\n",
    "#     print(expectedPDF)\n",
    "    retPDF = df.toPandas()\n",
    "   \n",
    "\n",
    "    # spark df is int32, pandas from dict is int64\n",
    "    pd.testing.assert_frame_equal( retPDF, expectedPDF, check_dtype=False )\n",
    "    \n",
    "testConvertToInt( countsSparkDF )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grave-discovery",
   "metadata": {},
   "source": [
    "# Step 5) pre filter\n",
    "saves a lot of memory\n",
    "\n",
    "<span style=\"color:red\">Not needed. is this computationly efficent. does this mess up row sums?</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "instant-desire",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from operator import add\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "def rowSums( countsSparkDF, columnNames ):\n",
    "    # https://stackoverflow.com/a/54283997/4586180\n",
    "    retDF = countsSparkDF.na.fill(0).withColumn(\"rowSum\" ,reduce(add, [col(x) for x in columnNames]))\n",
    "    return retDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "concerned-thought",
   "metadata": {},
   "source": [
    "# Step 6) estimate Scaling Factors for each sample\n",
    "account for differences in library size and library composition.\n",
    "\n",
    "library composition issue, we want to compare two sample with the same size. Assume in disease/knock out one gene is not expessed. This results in the remaining genes getting more counts than otherwise. I.e. the extra reads for the knock out gene get distributed to the remaining genes\n",
    "\n",
    "## DESeq implementation</span>\n",
    "<span style=\"color:red\"> This will not work in spark</span>\n",
    "```\n",
    "a) calculate the log of all values\n",
    "    i) logs are no easily swad by outliners\n",
    "    ii) we may have zero counts, log is undefined\n",
    "    \n",
    "b) calculate the 'geometic average' of the rows\n",
    "    i) mean( rowsum )\n",
    "    \n",
    "c) filter out genes with average = +/- infinity \n",
    "    i) removes genes with zero in one or more samples. \n",
    "    that are type specific. I.e. we want to focus on the house keeping genes.\n",
    "    These are genes that are trascripted at simpilar levels regradless of tissue type\n",
    "    \n",
    "d) subtract the avereage log values from the log(counts)\n",
    "    i) this is equal to log( numRead_x / average numRead_x)\n",
    "    \n",
    "e) calculate the median of the ratio for each sample \n",
    "    i) median is robust\n",
    "    \n",
    "f) convert the medians back to linear scale\n",
    "```\n",
    "\n",
    "## Spark implementation\n",
    "DESeq is implemented in R and uses an R implementation detail to remove genes with zero in one or more samples. This will not work in spark. R defines log(0) = -inf. All arithimetic operations on -inf = -inf. DESeq calculates log of all values then calculates the row means. It then removes rows where row mean = -inf.\n",
    "\n",
    "In spark log(0) is defined to null. It seems like spark will treat null as zero. That is to say that if some genes have zero counts the rowSums will not be -inf. We will need to filter these genes out before calculating row means.\n",
    "\n",
    "```\n",
    "a) calculate the log of all values\n",
    "    i) logs are no easily swad by outliners\n",
    "    ii) we may have zero counts, log is undefined. These values will be null\n",
    "    \n",
    "b) filter out genes with one or more nulls\n",
    "   i) removes genes with zero in one or more samples. \n",
    "    that are type specific. I.e. we want to focus on the house keeping genes.\n",
    "    These are genes that are trascripted at simpilar levels regradless of tissue type\n",
    "    \n",
    "c) calculate the 'geometic average' of the rows\n",
    "    i) mean( rowsum )\n",
    "\n",
    "d) subtract the avereage log values from the log(counts)\n",
    "    i) this is equal to log( numRead_x / average numRead_x)\n",
    "    \n",
    "e) calculate the median of the ratio for each sample \n",
    "    i) median is robust\n",
    "    \n",
    "f) convert the medians back to linear scale\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "stupid-antibody",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "|geneId|       log(ctrl_1)|       log(ctrl_2)|       log(ctrl_3)|       log(kras_1)|       log(kras_2)|       log(kras_3)|\n",
      "+------+------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "|gene_2|2.3978952727983707|2.3978952727983707|2.3978952727983707| 4.700480365792417| 4.700480365792417| 4.700480365792417|\n",
      "|gene_7| 2.772588722239781| 2.772588722239781| 2.772588722239781| 5.075173815233827| 5.075173815233827| 5.075173815233827|\n",
      "|gene_6|  2.70805020110221|  2.70805020110221|  2.70805020110221|5.0106352940962555|5.0106352940962555|5.0106352940962555|\n",
      "|gene_4|2.5649493574615367|2.5649493574615367|2.5649493574615367| 4.867534450455582| 4.867534450455582| 4.867534450455582|\n",
      "|gene_1|              null|              null|              null|              null|              null|              null|\n",
      "|gene_5|2.6390573296152584|2.6390573296152584|2.6390573296152584| 4.941642422609304| 4.941642422609304| 4.941642422609304|\n",
      "|gene_3|2.4849066497880004|2.4849066497880004|              null| 4.787491742782046| 4.787491742782046| 4.787491742782046|\n",
      "|gene_8|3.9889840465642745|3.9889840465642745| 4.007333185232471|  6.29156913955832|  6.29156913955832| 6.293419278846481|\n",
      "+------+------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6.a) calculate log of all counts\n",
    "from pyspark.sql.functions import log\n",
    "\n",
    "# skip first column, i.e. gene_id\n",
    "columns = countsSparkDF.columns[1:]\n",
    "colNameAndAlias = ( (c, c.replace('sum', \"log\"))  for c in columns )\n",
    "\n",
    "logCountsSparkDF = countsSparkDF.select( col('geneId') ,\n",
    "                                        *(log(c).alias(a) for c,a in colNameAndAlias ) ) \n",
    "                                           \n",
    "logCountsSparkDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "distant-halloween",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testConvertToLog( df ):\n",
    "    testPDF = df.select(['geneId', 'log(ctrl_1)', 'log(kras_3)']).toPandas()\n",
    "#     print(testPDF)\n",
    "#     print(testPDF.to_dict())\n",
    "\n",
    "    expectedPDFDict = {\n",
    "     'geneId': {0: 'gene_2', 1: 'gene_7', 2: 'gene_6', 3: 'gene_4', 4: 'gene_1', 5: 'gene_5', 6: 'gene_3', 7: 'gene_8'}, \n",
    "     'log(ctrl_1)': {0: 2.3978952727983707, 1: 2.772588722239781, 2: 2.70805020110221,   3: 2.5649493574615367, 4: 'nan', 5: 2.6390573296152584, 6: 2.4849066497880004, 7: 3.9889840465642745}, \n",
    "     'log(kras_3)': {0: 4.700480365792417,  1: 5.075173815233827, 2: 5.0106352940962555, 3: 4.867534450455582,  4: 'nan', 5: 4.941642422609304,  6: 4.787491742782046,  7: 6.293419278846481}\n",
    "    }\n",
    "\n",
    "    import numpy as np\n",
    "\n",
    "    # print(expectedPDF)\n",
    "    expectedPDF = pd.DataFrame( expectedPDFDict )\n",
    "    selectRows = expectedPDF.loc[:,\"geneId\"] == \"gene_1\"\n",
    "    expectedPDF.loc[selectRows, [\"log(ctrl_1)\", \"log(kras_3)\"] ] = np.NaN #pd.NaN\n",
    "    #print(expectedPDF)\n",
    "\n",
    "    # expectedPDF\n",
    "\n",
    "    pd.testing.assert_frame_equal( testPDF, expectedPDF, check_dtype=False )\n",
    "    \n",
    "testConvertToLog( logCountsSparkDF )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "romantic-jumping",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 6.b) filter out genes with one or more nulls\n",
    "#    i) removes genes with zero in one or more samples. \n",
    "#     that are type specific. I.e. we want to focus on the house keeping genes.\n",
    "#     These are genes that are trascripted at simpilar levels regradless of tissue type\n",
    "\n",
    "filteredDF = logCountsSparkDF.na.drop()\n",
    "\n",
    "def testFilter( df ) :\n",
    "    filteredPDF = df.select(\"geneId\").toPandas()\n",
    "    # expectedDict = retPDF.to_dict()\n",
    "    # print(expectedDict)\n",
    "    expectedDict = {'geneId': {0: 'gene_2', 1: 'gene_7', 2: 'gene_6', 3: 'gene_4', 4: 'gene_5', 5: 'gene_8'}}\n",
    "\n",
    "    expectedPDF = pd.DataFrame( expectedDict )\n",
    "    pd.testing.assert_frame_equal(filteredPDF , expectedPDF )\n",
    "    \n",
    "testFilter( filteredDF )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "sitting-indianapolis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "|geneId|       log(ctrl_1)|       log(ctrl_2)|       log(ctrl_3)|       log(kras_1)|       log(kras_2)|       log(kras_3)|            rowSum|\n",
      "+------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "|gene_2|2.3978952727983707|2.3978952727983707|2.3978952727983707| 4.700480365792417| 4.700480365792417| 4.700480365792417| 21.29512691577236|\n",
      "|gene_7| 2.772588722239781| 2.772588722239781| 2.772588722239781| 5.075173815233827| 5.075173815233827| 5.075173815233827| 23.54328761242082|\n",
      "|gene_6|  2.70805020110221|  2.70805020110221|  2.70805020110221|5.0106352940962555|5.0106352940962555|5.0106352940962555|23.156056485595393|\n",
      "|gene_4|2.5649493574615367|2.5649493574615367|2.5649493574615367| 4.867534450455582| 4.867534450455582| 4.867534450455582|22.297451423751358|\n",
      "|gene_5|2.6390573296152584|2.6390573296152584|2.6390573296152584| 4.941642422609304| 4.941642422609304| 4.941642422609304| 22.74209925667369|\n",
      "|gene_8|3.9889840465642745|3.9889840465642745| 4.007333185232471|  6.29156913955832|  6.29156913955832| 6.293419278846481|30.861858836324142|\n",
      "+------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6.c) calculate the mean of the row sum\n",
    "# skip gene_id\n",
    "columns = filteredDF.columns[1:]\n",
    "rowSumsDF = rowSums( filteredDF, columns )\n",
    "rowSumsDF.show()\n",
    "\n",
    "# rowSumsDF.select([\"geneId\", \"rowSum\"]).show()\n",
    "# rowSumsPDF = rowSumsDF.select([\"geneId\", \"rowSum\"]).toPandas()\n",
    "#print(rowSumsPDF.to_dict())\n",
    "\n",
    "def testRowSum( df ):\n",
    "    df.select([\"geneId\", \"rowSum\"]) #.show()\n",
    "    pdf = rowSumsDF.select([\"geneId\", \"rowSum\"]).toPandas()\n",
    "    #print(pdf.to_dict())\n",
    "    \n",
    "    expectedDict =  {\n",
    "        'geneId': {0: 'gene_2', 1: 'gene_7', 2: 'gene_6', 3: 'gene_4', 4: 'gene_5', 5: 'gene_8'}, \n",
    "        'rowSum': {0: 21.29512691577236, 1: 23.54328761242082, 2: 23.156056485595393, 3: 22.297451423751358, 4: 22.74209925667369, 5: 30.861858836324142}\n",
    "    }\n",
    "    \n",
    "    expectedPDF = pd.DataFrame( expectedDict )\n",
    "\n",
    "    pd.testing.assert_frame_equal(pdf , expectedPDF )\n",
    "    \n",
    "testRowSum(rowSumsDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "breeding-constraint",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "|geneId|       log(ctrl_1)|       log(ctrl_2)|       log(ctrl_3)|       log(kras_1)|       log(kras_2)|       log(kras_3)|            rowSum|           rowMean|\n",
      "+------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "|gene_2|2.3978952727983707|2.3978952727983707|2.3978952727983707| 4.700480365792417| 4.700480365792417| 4.700480365792417| 21.29512691577236|3.5491878192953936|\n",
      "|gene_7| 2.772588722239781| 2.772588722239781| 2.772588722239781| 5.075173815233827| 5.075173815233827| 5.075173815233827| 23.54328761242082| 3.923881268736803|\n",
      "|gene_6|  2.70805020110221|  2.70805020110221|  2.70805020110221|5.0106352940962555|5.0106352940962555|5.0106352940962555|23.156056485595393| 3.859342747599232|\n",
      "|gene_4|2.5649493574615367|2.5649493574615367|2.5649493574615367| 4.867534450455582| 4.867534450455582| 4.867534450455582|22.297451423751358|3.7162419039585597|\n",
      "|gene_5|2.6390573296152584|2.6390573296152584|2.6390573296152584| 4.941642422609304| 4.941642422609304| 4.941642422609304| 22.74209925667369| 3.790349876112282|\n",
      "|gene_8|3.9889840465642745|3.9889840465642745| 4.007333185232471|  6.29156913955832|  6.29156913955832| 6.293419278846481|30.861858836324142| 5.143643139387357|\n",
      "+------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n = len( rowSumsDF.columns ) -2 # do not count geneId or rowSum columns\n",
    "rowMeansDF = rowSumsDF.withColumn(\"rowMean\", (rowSumsDF.rowSum / n ))\n",
    "rowMeansDF.show()\n",
    "\n",
    "# AEDWIP should be \" use RStudio symbolic debugger\"\n",
    "# # Browse[7]> loggeomeans\n",
    "# #   gene_1   gene_2   gene_3    gene_4    gene_5    gene_6    gene_7    gene_8 \n",
    "# #     -Inf 3.549188     -Inf   3.716242 3.790350  3.859343  3.923881  5.143643\n",
    "\n",
    "# fix unit test. we corrected teh bug but did not fix the unit test becase we wanted to\n",
    "# fix the int( round(count) ) bug first to save time\n",
    "\n",
    "def testRowMeans( df ):\n",
    "    df.select([\"geneId\", \"rowSum\", \"rowMean\"]) #.show() \n",
    "    rowMeansPDF = df.select([\"geneId\", \"rowSum\", \"rowMean\"]).toPandas()\n",
    "    \n",
    "    expectedDict = rowMeansPDF.to_dict()\n",
    "#     print(expectedDict)\n",
    "\n",
    "#     expectedDict = {\n",
    "#         'geneId':  {0: 'gene_2',          1: 'gene_7',           2: 'gene_6',           3: 'gene_4',           4: 'gene_5',           5: 'gene_8'}, \n",
    "#         'rowSum':  {0: 21.29512691577236, 1: 23.54328761242082,  2: 23.156056485595393, 3: 22.297451423751358, 4: 22.74209925667369,  5: 30.841659558367784}, \n",
    "#         'rowMean': {0: 2.661890864471545, 1: 2.9429109515526024, 2: 2.894507060699424,  3: 2.7871814279689198, 4: 2.8427624070842112, 5: 3.855207444795973}}\n",
    "\n",
    "    expectedDict = {\n",
    "        'geneId':  {0: 'gene_2',           1: 'gene_7',          2: 'gene_6',           3: 'gene_4',           4: 'gene_5',          5: 'gene_8'}, \n",
    "        'rowSum':  {0: 21.29512691577236,  1: 23.54328761242082, 2: 23.156056485595393, 3: 22.297451423751358, 4: 22.74209925667369, 5: 30.861858836324142}, \n",
    "        'rowMean': {0: 3.5491878192953936, 1: 3.923881268736803, 2: 3.859342747599232,  3: 3.7162419039585597, 4: 3.790349876112282, 5: 5.143643139387357}}\n",
    "   \n",
    "    expectedPDF = pd.DataFrame( expectedDict )\n",
    "    pd.testing.assert_frame_equal(rowMeansPDF , expectedPDF )\n",
    "    \n",
    "testRowMeans( rowMeansDF )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "revised-iraqi",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['log(ctrl_1)', 'log(ctrl_2)', 'log(ctrl_3)', 'log(kras_1)', 'log(kras_2)', 'log(kras_3)']\n",
      "+------+-------------------+-------------------+-------------------+------------------+------------------+------------------+\n",
      "|geneId|             ctrl_1|             ctrl_2|             ctrl_3|            kras_1|            kras_2|            kras_3|\n",
      "+------+-------------------+-------------------+-------------------+------------------+------------------+------------------+\n",
      "|gene_2| -1.151292546497023| -1.151292546497023| -1.151292546497023| 1.151292546497023| 1.151292546497023| 1.151292546497023|\n",
      "|gene_7| -1.151292546497022| -1.151292546497022| -1.151292546497022|1.1512925464970234|1.1512925464970234|1.1512925464970234|\n",
      "|gene_6| -1.151292546497022| -1.151292546497022| -1.151292546497022|1.1512925464970234|1.1512925464970234|1.1512925464970234|\n",
      "|gene_4| -1.151292546497023| -1.151292546497023| -1.151292546497023|1.1512925464970225|1.1512925464970225|1.1512925464970225|\n",
      "|gene_5|-1.1512925464970234|-1.1512925464970234|-1.1512925464970234| 1.151292546497022| 1.151292546497022| 1.151292546497022|\n",
      "|gene_8|-1.1546590928230822|-1.1546590928230822|-1.1363099541548856|1.1479260001709637|1.1479260001709637|1.1497761394591244|\n",
      "+------+-------------------+-------------------+-------------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6.d) subtract the avereage log values from the log(counts)\n",
    "#     i) this is equal to log( numRead_x / average numRead_x)\n",
    "\n",
    "def subtractRowMeanFromLogCounts( sparkDF, columnNames ):\n",
    "    colNameAndAlias = ( (c, c.replace('log(','').replace(')',''))  for c in columns )\n",
    "    retDF = sparkDF.select(  col('geneId'),\n",
    "                           *((col(c) - col('rowMean')).alias(a) for c,a in colNameAndAlias) )\n",
    "    return retDF\n",
    "\n",
    "\n",
    "# skip the first and last 2 columns, ie. geneId, rowSum, rowMean\n",
    "columnNames = rowMeansDF.columns[1:-2]\n",
    "print(columnNames)\n",
    "\n",
    "ratioDF = subtractRowMeanFromLogCounts( rowMeansDF, columnNames )\n",
    "ratioDF.show()\n",
    "\n",
    "def testRatio( df ):\n",
    "    ratioPDF = df.toPandas()\n",
    "    expectedDict = ratioPDF.to_dict()\n",
    "#     print(expectedDict)\n",
    "    \n",
    "    expectedDict = {\n",
    "        'geneId': {0: 'gene_2', 1: 'gene_7', 2: 'gene_6', 3: 'gene_4', 4: 'gene_5', 5: 'gene_8'}, \n",
    "        'ctrl_1': {0: -1.151292546497023, 1: -1.151292546497022, 2: -1.151292546497022, 3: -1.151292546497023, 4: -1.1512925464970234, 5: -1.1546590928230822}, \n",
    "        'ctrl_2': {0: -1.151292546497023, 1: -1.151292546497022, 2: -1.151292546497022, 3: -1.151292546497023, 4: -1.1512925464970234, 5: -1.1546590928230822}, \n",
    "        'ctrl_3': {0: -1.151292546497023, 1: -1.151292546497022, 2: -1.151292546497022, 3: -1.151292546497023, 4: -1.1512925464970234, 5: -1.1363099541548856}, \n",
    "        'kras_1': {0: 1.151292546497023, 1: 1.1512925464970234, 2: 1.1512925464970234, 3: 1.1512925464970225, 4: 1.151292546497022, 5: 1.1479260001709637}, \n",
    "        'kras_2': {0: 1.151292546497023, 1: 1.1512925464970234, 2: 1.1512925464970234, 3: 1.1512925464970225, 4: 1.151292546497022, 5: 1.1479260001709637}, \n",
    "        'kras_3': {0: 1.151292546497023, 1: 1.1512925464970234, 2: 1.1512925464970234, 3: 1.1512925464970225, 4: 1.151292546497022, 5: 1.1497761394591244}}\n",
    "    \n",
    "    expectedPDF = pd.DataFrame( expectedDict )\n",
    "\n",
    "    pd.testing.assert_frame_equal(ratioPDF , expectedPDF )\n",
    "    \n",
    "testRatio(ratioDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "looking-ambassador",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.calculate the median of the ratio for each sample \n",
    "#     i) median is robust\n",
    "\n",
    "def median( sparkDataFrame, columnNames ):\n",
    "    '''\n",
    "    https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.functions.percentile_approx.html\n",
    "    if you have an odd number of rows it does not calculate the value between the 2 middle values\n",
    "    '''\n",
    "    \n",
    "    retDF = sparkDataFrame.select( *(sqlFunc.percentile_approx( c, 0.5, accuracy=1000000 ) \n",
    "                                     for c in columnNames) )\n",
    "    return retDF\n",
    "\n",
    "def testMedian() :\n",
    "    def singleRowDFToNumpyArray( sparkDF ):\n",
    "        pdf = sparkDF.toPandas()\n",
    "        np = pdf.values[0]  \n",
    "        return np\n",
    "\n",
    "    # test even number of rows\n",
    "    evenPDF = pd.DataFrame( {\n",
    "        \"a\" : [i * 1.0 for i in range(6)],\n",
    "        \"b\" : [i * 2.0 for i in range(6)],\n",
    "    })\n",
    "    evenSparkDF = spark.createDataFrame( evenPDF )\n",
    "    resultDF = median( evenSparkDF, evenSparkDF.columns )\n",
    "    resultMediansNP = singleRowDFToNumpyArray( resultDF )\n",
    "\n",
    "    # test odd number of rows\n",
    "    expectMedianNP = np.array( [2., 4.] )\n",
    "    np.testing.assert_array_equal(resultMediansNP, expectMedianNP)\n",
    "\n",
    "    oddPDF = pd.DataFrame( {\n",
    "        \"c\" : [i * 1.0 for i in range(7)],\n",
    "        \"d\" : [i * 2.0 for i in range(7)],\n",
    "    })\n",
    "    oddSparkDF = spark.createDataFrame( oddPDF )\n",
    "    resultDF = median( oddSparkDF, oddSparkDF.columns )\n",
    "    resultMediansNP = singleRowDFToNumpyArray( resultDF )\n",
    "    expectMedianNP = np.array( [3., 6.] )\n",
    "    np.testing.assert_array_equal(resultMediansNP, expectMedianNP)\n",
    "    \n",
    "testMedian()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "better-publication",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip geneId\n",
    "columnNames = ratioDF.columns[1:]\n",
    "estimatedScalingFactorsDF = median( ratioDF, columnNames )\n",
    "#print(estimatedScalingFactorsDF.columns)\n",
    "\n",
    "# rename columns\n",
    "def getSampleNames(colName):\n",
    "    #x = \"percentile_approx(ctrl_1, 0.5, 1000000)\"\n",
    "    #name = x.split(\"(\")[1].split(\",\")[0]\n",
    "    ret = colName.split(\"(\")[1].split(\",\")[0]\n",
    "#     print(ret)\n",
    "    return ret\n",
    "\n",
    "newColNames = [getSampleNames(c) for c in estimatedScalingFactorsDF.columns]\n",
    "logScalingFactorsDF = estimatedScalingFactorsDF.toDF( *newColNames )\n",
    "\n",
    "def testLogScaleFactor( df ):\n",
    "    logScalingFactorsPDF = df.toPandas()\n",
    "#     print(logScalingFactorsPDF.to_dict())\n",
    "    \n",
    "    expectedDict = {'ctrl_1': {0: -1.151292546497023}, 'ctrl_2': {0: -1.151292546497023}, \n",
    "                     'ctrl_3': {0: -1.151292546497023}, 'kras_1': {0: 1.1512925464970225}, \n",
    "                     'kras_2': {0: 1.1512925464970225}, 'kras_3': {0: 1.1512925464970225}}\n",
    "    \n",
    "    expectedPDF = pd.DataFrame( expectedDict )\n",
    "    pd.testing.assert_frame_equal(logScalingFactorsPDF , expectedPDF )\n",
    "    \n",
    "testLogScaleFactor( logScalingFactorsDF )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "enclosed-martin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logScalingFactorsDF\n",
      "+------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "|            ctrl_1|            ctrl_2|            ctrl_3|            kras_1|            kras_2|            kras_3|\n",
      "+------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "|-1.151292546497023|-1.151292546497023|-1.151292546497023|1.1512925464970225|1.1512925464970225|1.1512925464970225|\n",
      "+------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "\n",
      "\n",
      "scalingFactorsDF\n",
      "+------------------+------------------+------------------+-----------------+-----------------+-----------------+\n",
      "|       EXP(ctrl_1)|       EXP(ctrl_2)|       EXP(ctrl_3)|      EXP(kras_1)|      EXP(kras_2)|      EXP(kras_3)|\n",
      "+------------------+------------------+------------------+-----------------+-----------------+-----------------+\n",
      "|0.3162277660168379|0.3162277660168379|0.3162277660168379|3.162277660168378|3.162277660168378|3.162277660168378|\n",
      "+------------------+------------------+------------------+-----------------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6.f) convert the medians back to linear scale\n",
    "print(\"logScalingFactorsDF\")\n",
    "logScalingFactorsDF.show()\n",
    "\n",
    "scalingFactorsDF = logScalingFactorsDF.select( *(sqlFunc.exp( c ) \n",
    "                                     for c in logScalingFactorsDF.columns) )\n",
    "print(\"\\nscalingFactorsDF\")\n",
    "scalingFactorsDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "southern-shaft",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to a single file\n",
    "# estimatedScalingFactorsOutFile = rootOutDir.joinpath(\"estimatedScalingFactors.csv\")\n",
    "# scalingFactorsDF.coalesce(1).write.csv( estimatedScalingFactorsOutFile.as_posix(), \n",
    "#                                                 mode='overwrite', header=True)\n",
    "\n",
    "\n",
    "def testScalingFactor( df ):\n",
    "    scalingFactorsPDF = scalingFactorsDF.toPandas()\n",
    "    expectedDict = scalingFactorsPDF.to_dict()\n",
    "    #print( expectedDict)\n",
    "\n",
    "    \n",
    "    # DESeq scaling factors. \n",
    "    # used RStudio symbolic debuger\n",
    "    # Browse[7]> sf\n",
    "    # [1] 0.3162278 0.3162278 0.3162278 3.1622777 3.1622777 3.1622777  \n",
    "    \n",
    "    expectedDict = {'EXP(ctrl_1)': {0: 0.3162277660168379}, 'EXP(ctrl_2)': {0: 0.3162277660168379}, \n",
    "                    'EXP(ctrl_3)': {0: 0.3162277660168379}, 'EXP(kras_1)': {0: 3.162277660168378}, \n",
    "                    'EXP(kras_2)': {0: 3.162277660168378},  'EXP(kras_3)': {0: 3.162277660168378}}\n",
    "\n",
    "    expectedPDF = pd.DataFrame( expectedDict )\n",
    "    pd.testing.assert_frame_equal(scalingFactorsPDF , expectedPDF )\n",
    "\n",
    "testScalingFactor(scalingFactorsDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serial-firewall",
   "metadata": {},
   "source": [
    "## Save scaling factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "gross-advertiser",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def saveScalingFactors( df, outfile ):\n",
    "    def getSampleNames(pdf):\n",
    "        #x = \"EXP(ctrl_1)\"\n",
    "        columnNames = pdf.columns.to_list()\n",
    "        ret = [ c.replace( \"EXP(\", \"\" ).replace( \")\",\"\" ) for c in columnNames ] \n",
    "    \n",
    "        return ret\n",
    "        \n",
    "    # this should easily fit into driver memory\n",
    "    # we want to save as 2 columns. It easier to transpose in pandas\n",
    "    scalingFactorsPDF = df.toPandas()\n",
    "    sampleNames = getSampleNames( scalingFactorsPDF )    \n",
    "#     print(\"sampleNames:{}\".format(sampleNames))\n",
    "    scalingFactorsPDF = scalingFactorsPDF.transpose()\n",
    "    \n",
    "    # replace \"EXP(sample name ) row names with sample name\"\n",
    "    scalingFactorsPDF = scalingFactorsPDF.set_axis( sampleNames, axis='index')\n",
    "    \n",
    "    scalingFactorsPDF.to_csv(scalingFactorsOutFile, sep=\"\\t\", index=True, header=False)\n",
    "    \n",
    "scalingFactorsOutFile = rootOutDir.joinpath(\"estimatedScalingFactors.tsv\")\n",
    "saveScalingFactors(scalingFactorsDF , scalingFactorsOutFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "handy-bicycle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ctrl_1\t0.3162277660168379\r\n",
      "ctrl_2\t0.3162277660168379\r\n",
      "ctrl_3\t0.3162277660168379\r\n",
      "kras_1\t3.162277660168378\r\n",
      "kras_2\t3.162277660168378\r\n",
      "kras_3\t3.162277660168378\r\n"
     ]
    }
   ],
   "source": [
    "! cat $scalingFactorsOutFile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "higher-laugh",
   "metadata": {},
   "source": [
    "## to scale, divide  all original read counts by scalling factors\n",
    "we need to scale all the genes in original count matrix. This includes\n",
    "genes we filtered out when calculated our scaling factors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arranged-discrimination",
   "metadata": {},
   "source": [
    "# Create master count spark data frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "familiar-contemporary",
   "metadata": {},
   "source": [
    "# invoke an action\n",
    "This will cause the optimized query plan to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "environmental-price",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#masterCountSDF.show(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southern-decision",
   "metadata": {},
   "source": [
    "# Save gene count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "loose-hobby",
   "metadata": {},
   "outputs": [],
   "source": [
    "outFile = rootOutDir.joinpath(\"txidGroupedByGeneidCounts\")\n",
    "partsOutFile = rootOutDir.joinpath(\"txidGroupedByGeneidCountsParts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "retired-agency",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixColNames( countDF ) :\n",
    "    '''\n",
    "    fix column names. the should be sample names\n",
    "    # 'sum(ctrl_1)'' should be 'ctrl_1'\n",
    "    '''\n",
    "    \n",
    "    # skip the the geneId colummn It is value like gene_2\n",
    "    oldNames = countDF.columns[1:]\n",
    "\n",
    "    sampleNames = ( (c, c.replace('sum(','').replace(')',''))  for c in oldNames )\n",
    "    # print(*sampleColNames)\n",
    "\n",
    "    retDF = countDF.select( col('geneId'), \n",
    "                           *((col(c).alias(a) for c,a in sampleNames) ) ) \n",
    "    return( retDF )\n",
    "                           \n",
    "countsSparkDF = fixColNames(countsSparkDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "intelligent-finance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# countsSparkDF.rdd.getNumPartitions()\n",
    "countsSparkDF.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "offensive-input",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved: testData/sparkDESeqTest2/output/txidGroupedByGeneidCountsParts\n",
      "+------+------+------+------+------+------+------+\n",
      "|geneId|ctrl_1|ctrl_2|ctrl_3|kras_1|kras_2|kras_3|\n",
      "+------+------+------+------+------+------+------+\n",
      "|gene_2|    11|    11|    11|   110|   110|   110|\n",
      "|gene_7|    16|    16|    16|   160|   160|   160|\n",
      "|gene_6|    15|    15|    15|   150|   150|   150|\n",
      "|gene_4|    13|    13|    13|   130|   130|   130|\n",
      "|gene_1|     0|     0|     0|     0|     0|     0|\n",
      "|gene_5|    14|    14|    14|   140|   140|   140|\n",
      "|gene_3|    12|    12|     0|   120|   120|   120|\n",
      "|gene_8|    54|    54|    55|   540|   540|   541|\n",
      "+------+------+------+------+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# write to 2 files\n",
    "# deseq balks if there are three files\n",
    "# Error in estimateDispersionsFit(object, fitType = fitType, quiet = quiet) : \n",
    "#   all gene-wise dispersion estimates are within 2 orders of magnitude\n",
    "#   from the minimum value, and so the standard curve fitting techniques will not work.\n",
    "#   One can instead use the gene-wise estimates as final estimates:\n",
    "#   dds <- estimateDispersionsGeneEst(dds)\n",
    "#   dispersions(dds) <- mcols(dds)$dispGeneEst\n",
    "#   ...then continue with testing using nbinomWaldTest or nbinomLRT\n",
    "\n",
    "numFiles = 3\n",
    "# countsSparkDF.coalesce(numFiles) \\\n",
    "hack = 5 # when we get here there is only one partition because of show()\n",
    "countsSparkDF.repartition(hack).coalesce(numFiles) \\\n",
    "    .write \\\n",
    "    .csv( partsOutFile.as_posix(), mode='overwrite', sep='\\t', header=True)\n",
    "\n",
    "print(\"saved: {}\".format(partsOutFile))\n",
    "# masterCountSDF.show()\n",
    "countsSparkDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "established-eleven",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# masterCountSDF.rdd.getNumPartitions()\n",
    "countsSparkDF.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "automated-navigation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved: testData/sparkDESeqTest2/output/txidGroupedByGeneidCounts\n"
     ]
    }
   ],
   "source": [
    "# write to one file\n",
    "# write to a single file\n",
    "# masterCountSDF.coalesce(1).write.csv( masterCountsOutFile.as_posix(), mode='overwrite', sep='\\t', header=True)\n",
    "countsSparkDF.coalesce(1).write.csv( outFile.as_posix(), mode='overwrite', sep='\\t', header=True)\n",
    "\n",
    "print(\"saved: {}\".format(outFile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "plain-finance",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-35-3f5b518eb8f8>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-35-3f5b518eb8f8>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    ugly hack to try and get data into file DESeq will not complain about\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "ugly hack to try and get data into file DESeq will not complain about\n",
    "the partitions are not of equal size\n",
    "\n",
    "does not fix bug\n",
    "\n",
    "# copy the last row from one file to the other\n",
    "\n",
    "$ tail -n 1 part-00000-3cb6ac58-373a-44ad-90ea-62712a2bc6b8-c000.csv >> \\\n",
    "    part-00001-3cb6ac58-373a-44ad-90ea-62712a2bc6b8-c000.csv \n",
    "\n",
    "# remove the last row\n",
    "$ wc -l part-00000-b3fd4a3f-bcf3-4dcf-8eea-03f2742ff9b3-c000.csv \n",
    "6 part-00000-b3fd4a3f-bcf3-4dcf-8eea-03f2742ff9b3-c000.csv\n",
    "\n",
    "$ head -n 5 part-00000-b3fd4a3f-bcf3-4dcf-8eea-03f2742ff9b3-c000.csv > \\\n",
    "    part-00000-b3fd4a3f-bcf3-4dcf-8eea-03f2742ff9b3-c000.csv\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
