{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7d70e0f",
   "metadata": {},
   "source": [
    "# concatenate Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "555c4d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPARK_HOME=\"../../sparkBin/spark-3.1.2-bin-hadoop3.2\"\n",
    "SPARK_HOME=\"../../terra/deseq/spark-3.1.2-bin-hadoop3.2\"\n",
    "import findspark\n",
    "findspark.init( SPARK_HOME )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00670cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"DESeqMasterETL\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d1196e",
   "metadata": {},
   "source": [
    "## load our mock terra data model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34997441",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleFile = \"testData/sample.tsv\"\n",
    "samplePDF = pd.read_csv(sampleFile, sep=\"\\t\")\n",
    "\n",
    "fileLst = samplePDF.loc[:, 'quant.sf'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98c16d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testData/ctrl.1.quant.sf\n",
      "testData/ctrl.2.quant.sf\n",
      "testData/ctrl.3.quant.sf\n",
      "testData/kras.1.quant.sf\n",
      "testData/kras.2.quant.sf\n",
      "testData/kras.3.quant.sf\n"
     ]
    }
   ],
   "source": [
    "# pre allocate slots to store data frames in\n",
    "quantSDFs = [None] * len(fileLst)\n",
    "quantSchema = \"`Name` STRING, `Length` INT, `EffectiveLength` DOUBLE, `Tmp` DOUBLE, `NumReads` DOUBLE \"\n",
    "\n",
    "for i in range( len(quantSDFs) ):\n",
    "    quantFile = \"testData/{}\".format( fileLst[i] )\n",
    "    print(quantFile)\n",
    "    df = spark.read.load( quantFile, format=\"csv\", sep=\"\\t\", \n",
    "                             schema=quantSchema, header=\"true\")\n",
    "    quantSDFs[i] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b81a43c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+---------------+---------+--------+\n",
      "|                Name|Length|EffectiveLength|      Tmp|NumReads|\n",
      "+--------------------+------+---------------+---------+--------+\n",
      "|ENST00000456328.2...|  1657|         1530.0|      0.0|     0.0|\n",
      "|ENST00000450305.2...|   632|          505.0|      0.0|     0.0|\n",
      "|ENST00000488147.1...|  1351|         1224.0|      0.0|     0.0|\n",
      "|ENST00000619216.1...|    68|           15.0|      0.0|     0.0|\n",
      "|ENST00000473358.1...|   712|          585.0|      0.0|     0.0|\n",
      "|ENST00000469289.1...|   535|          408.0|      0.0|     0.0|\n",
      "|ENST00000607096.1...|   138|           55.0|      0.0|     0.0|\n",
      "|ENST00000417324.1...|  1187|         1060.0|      0.0|     0.0|\n",
      "|ENST00000461467.1...|   590|         885.84| 5.813169|    4.93|\n",
      "|ENST00000606857.1...|   840|          713.0|      0.0|     0.0|\n",
      "|ENST00000642116.1...|  1414|         1287.0|      0.0|     0.0|\n",
      "|ENST00000492842.2...|   939|          812.0|      0.0|     0.0|\n",
      "|ENST00000641515.2...|  2618|         2491.0|      0.0|     0.0|\n",
      "|ENST00000335137.4...|  1054|         348.69| 3.815162|   1.274|\n",
      "|ENST00000466430.5...|  2748|       3036.339|10.847308|  31.533|\n",
      "|ENST00000477740.5...|   491|          365.0|      0.0|     0.0|\n",
      "|ENST00000471248.1...|   629|        708.813|61.221885|  41.547|\n",
      "|ENST00000610542.1...|   723|          596.0|      0.0|     0.0|\n",
      "|ENST00000453576.2...|   336|          214.0|      0.0|     0.0|\n",
      "+--------------------+------+---------------+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "quantSDFs[0].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e08eee4",
   "metadata": {},
   "source": [
    "# Test 1 can we use dataframe withColumn to add a column to another dataframe?\n",
    "# <span style=\"color:red\">NO </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f22f5970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|                Name|NumReads|\n",
      "+--------------------+--------+\n",
      "|ENST00000456328.2...|     0.0|\n",
      "|ENST00000450305.2...|     0.0|\n",
      "|ENST00000488147.1...|     0.0|\n",
      "|ENST00000619216.1...|     0.0|\n",
      "|ENST00000473358.1...|     0.0|\n",
      "+--------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = quantSDFs[0].select(\"Name\", \"NumReads\").limit(5)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "842f8300",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|NumReads|\n",
      "+--------+\n",
      "|     4.0|\n",
      "|     0.0|\n",
      "|     0.0|\n",
      "|     0.0|\n",
      "|     0.0|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = quantSDFs[-2].select(\"NumReads\").limit(5)\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "89f63a4a",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "withColumn() missing 1 required positional argument: 'col'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-e335055331d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# df3 = df1.withColumn( df2.col(\"NumReads\"))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# df3 = df1.withColumn( df2[\"NumReads\"])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mdf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNumReads\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdf3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: withColumn() missing 1 required positional argument: 'col'"
     ]
    }
   ],
   "source": [
    "# df3 = df1.withColumn( df2.col(\"NumReads\"))\n",
    "# df3 = df1.withColumn( df2[\"NumReads\"])\n",
    "df3 = df1.withColumn( df2.NumReads )\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ad114119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.column.Column'>\n"
     ]
    }
   ],
   "source": [
    "df2.NumReads\n",
    "print(type(df2.NumReads))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f000a406",
   "metadata": {},
   "source": [
    "# test 2 can we use pivot tables and union?\n",
    "<span style=\"color:red\">???</span>\n",
    "- http://www.svds.com/pivoting-data-in-sparksql/\n",
    "- https://databricks.com/blog/2016/02/09/reshaping-data-with-pivot-in-apache-spark.html\n",
    "- https://spark.apache.org/docs/1.6.0/api/java/org/apache/spark/sql/GroupedData.html\n",
    "\n",
    "try\n",
    "- https://stackoverflow.com/questions/40892459/spark-transpose-dataframe-without-aggregating\n",
    "\n",
    "take a look at coordinate matrix and block matrix\n",
    "- https://spark.apache.org/docs/latest/api/java/org/apache/spark/mllib/linalg/distributed/CoordinateMatrix.html\n",
    "- https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.mllib.linalg.distributed.BlockMatrix.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2877e980",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "47000329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+--------+\n",
      "|                Name|Tmp|NumReads|\n",
      "+--------------------+---+--------+\n",
      "|ENST00000456328.2...|0.0|     0.0|\n",
      "|ENST00000450305.2...|0.0|     0.0|\n",
      "|ENST00000488147.1...|0.0|     0.0|\n",
      "|ENST00000619216.1...|0.0|     0.0|\n",
      "|ENST00000473358.1...|0.0|     0.0|\n",
      "+--------------------+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = quantSDFs[0].select(\"Name\", \"Tmp\", \"NumReads\").limit(5)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1b32f826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+--------+\n",
      "|                Name|     Tmp|NumReads|\n",
      "+--------------------+--------+--------+\n",
      "|ENST00000456328.2...|0.727248|   1.998|\n",
      "|ENST00000450305.2...|     0.0|     0.0|\n",
      "|ENST00000488147.1...|     0.0|     0.0|\n",
      "|ENST00000619216.1...|     0.0|     0.0|\n",
      "|ENST00000473358.1...|     0.0|     0.0|\n",
      "+--------------------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = quantSDFs[2].select(\"Name\", \"Tmp\", \"NumReads\").limit(5)\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "411a4f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+-----+\n",
      "|                Name| 0.0|1.998|\n",
      "+--------------------+----+-----+\n",
      "|ENST00000456328.2...|null|1.998|\n",
      "|ENST00000450305.2...| 0.0| null|\n",
      "|ENST00000488147.1...| 0.0| null|\n",
      "|ENST00000619216.1...| 0.0| null|\n",
      "|ENST00000473358.1...| 0.0| null|\n",
      "+--------------------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df1p = df1.pivot(\"NumReads\")\n",
    "df2p = df2.groupBy(\"Name\").pivot(\"NumReads\").avg(\"NumReads\")\n",
    "df2p.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ddded2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
